{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89211c-0929-4e6b-aa02-850392ca0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source /scratch/phys/sin/sethih1/venv/MolNexTR_env/bin/activate\n",
    "import sys\n",
    "sys.path.append('/home/sethih1/MORAFInator/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebba21e6-b9be-4aa4-887f-1fa9cfb33606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "COVALENT_RADII = {\n",
    "    1: 0.32,\n",
    "    6: 0.77,\n",
    "    7: 0.75,\n",
    "    8: 0.73,\n",
    "    9: 0.71,\n",
    "}\n",
    "\n",
    "ELEMENT_TO_INDEX = {\n",
    "    1: 0,\n",
    "    6: 1,\n",
    "    7: 2,\n",
    "    8: 3,\n",
    "    9: 4,\n",
    "}\n",
    "\n",
    "INDEX_TO_SYMBOL = {\n",
    "    1: 'H',\n",
    "    6: 'C',\n",
    "    7: 'N',\n",
    "    8: 'O',\n",
    "    9: 'F',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "INDEX_TO_SYMBOL = {\n",
    "    0: 'H',\n",
    "    1: 'C',\n",
    "    2: 'N',\n",
    "    3: 'O',\n",
    "    4: 'F',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2e86746-1d6c-406e-af06-4c3e18c99326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFMData(Dataset): \n",
    "    def __init__(self, data_path, transform, train_size=0.8, split='train'): \n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        with h5py.File(self.data_path, 'r') as f:\n",
    "            total_length = f['x'].shape[0]\n",
    "            self.train_length = int(train_size*total_length)\n",
    "            self.val_length = total_length - self.train_length\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == 'train':\n",
    "            return self.train_length\n",
    "        else: \n",
    "            return self.val_length\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        if self.split == 'train': \n",
    "            idx += 0\n",
    "        else: \n",
    "            idx += self.train_length\n",
    "\n",
    "        with h5py.File(self.data_path, 'r') as f:\n",
    "            x = f['x'][idx]\n",
    "            xyz = f['xyz'][idx]\n",
    "\n",
    "        # Remove padding atoms\n",
    "        xyz = xyz[xyz[:, -1] > 0]\n",
    "\n",
    "        # Get edges \n",
    "        edges = []\n",
    "        for i in range(xyz.shape[0]): \n",
    "            for j in range(i+1, xyz.shape[0]): \n",
    "                dist = np.linalg.norm(xyz[i, :3] - xyz[j, :3])\n",
    "                if dist < 1.2*(COVALENT_RADII[xyz[i, -1]] + COVALENT_RADII[xyz[j, -1]]):\n",
    "                    edges.append([i,j])\n",
    "\n",
    "        # Normalize xyz to [0.25, 0.75]\n",
    "        xyzmin = np.min(xyz[:, :3])\n",
    "        xyz_max = np.max(xyz[:, 3])\n",
    "\n",
    "        xyz[:, :3] = (xyz[:, :3] - xyzmin[:, :3])/(xyz_max - xyzmin)\n",
    "        xyz[:, :3] = 0.5*xyz[:, :3] + 0.25\n",
    "\n",
    "        # map atom types to integers (0,1, ..)\n",
    "        xyz[:, -1] = [ELEMENT_TO_INDEX[atom_type] for atom_type in xyz[:, -1]]\n",
    "\n",
    "        sample = {'coords': xyz, edges: np.asarray(edges)}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # Keep all channels from HDF5 and convert to [C,H,W]\n",
    "        x = torch.from_numpy(x)\n",
    "        if x.dim() == 4 and x.size(0) == 1: \n",
    "            # Input like [1, H, W, C] -> [H, W, C]\n",
    "            x = x.squeeze(0)\n",
    "        if x.dim() == 3: \n",
    "            # Assume [H, W, C] from HDF5, move channels first\n",
    "            x = x.permute(2,0,1).continguous()\n",
    "\n",
    "        return idx, x, sample\n",
    "\n",
    "\n",
    "def get_datasets(data_path, train_transform = None, val_transform = None, train_size = 0.8): \n",
    "\n",
    "    train_dataset = AFMData(data_path, transform = train_transform, train_size=train_size, split='train')\n",
    "    val_dataset = AFMData(data_path, transform = val_transform, train_size=train_size, split='val')\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def afm_collate_fn(batch): \n",
    "\n",
    "    sample = {'coords': [], 'edges': []}\n",
    "    ids = [id[0] for id in batch]\n",
    "    images = torch.stack([item[1] for item in batch])\n",
    "    for item in batch:\n",
    "        sample['coords'].append(torch.from_numpy(item[2]['coords']))\n",
    "        sample['edges'].append(torch.from_numpy(item[2]['edges']))\n",
    "\n",
    "    return ids, images, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8722bf7-d926-46a9-b1ca-dd08d4c8a444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sw: <HDF5 dataset \"sw\": shape (59392, 1, 2, 3), type \"<f4\">\n",
      "x: <HDF5 dataset \"x\": shape (59392, 1, 128, 128, 10), type \"<f4\">\n",
      "xyz: <HDF5 dataset \"xyz\": shape (59392, 54, 5), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "afm_data = h5py.File(\"/scratch/phys/project/sin/hackathon/data/afm.h5\", 'r')\n",
    "for key in afm_data.keys(): \n",
    "    print(f\"{key}: {afm_data[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d31cb4c5-34c3-4636-88b6-3a40fb4b1566",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "9.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mask \u001b[38;5;241m=\u001b[39m afm_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxyz\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      2\u001b[0m new_data \u001b[38;5;241m=\u001b[39m afm_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxyz\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m,mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m atomtok \u001b[38;5;241m=\u001b[39m [INDEX_TO_SYMBOL[value] \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m new_data]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(atomtok)\n",
      "Cell \u001b[0;32mIn[48], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m mask \u001b[38;5;241m=\u001b[39m afm_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxyz\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      2\u001b[0m new_data \u001b[38;5;241m=\u001b[39m afm_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxyz\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m,mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m atomtok \u001b[38;5;241m=\u001b[39m [\u001b[43mINDEX_TO_SYMBOL\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m new_data]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(atomtok)\n",
      "\u001b[0;31mKeyError\u001b[0m: 9.0"
     ]
    }
   ],
   "source": [
    "mask = afm_data['xyz'][0, :, -1] > 0\n",
    "new_data = afm_data['xyz'][0,mask, -1]\n",
    "atomtok = [INDEX_TO_SYMBOL[value] for value in new_data]\n",
    "print(atomtok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a269ef5-fd34-4fe9-b02d-4634a6a81fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_bins = 64\n",
    "\n",
    "coord_bin_values = np.round(np.linspace(0,1,64),2)\n",
    "print(coord_bin_values)\n",
    "coord_tokens = [f'<COORD_{i}>' for i in range(coord_bins)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad0890e-7837-4b30-bb38-48dcc6ea28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_coord = coord_bin_values[np.argmin(np.abs(coord_bin_values.reshape(1,-1) - coord.reshape(-1,1)), axis = 1)]\n",
    "print(new_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195c00e-b5ce-4b72-9aa0-8f89e3f98522",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = afm_data['xyz'][0, mask, :3]\n",
    "coord_tokens = []\n",
    "coordo_tokens = []\n",
    "for symbol, coord in zip(atomtok, coords):\n",
    "    new_coord = coord_bin_values[np.argmin(np.abs(coord_bin_values.reshape(1,-1) - coord.reshape(-1,1)), axis = 1)]\n",
    "    coord_tokens.append(f'{symbol}: {new_coord[0]}, {new_coord[1]}, {new_coord[2]},')\n",
    "    coordo_tokens.append(f'{symbol}: {coord[0]}, {coord[1]}, {coord[2]},')\n",
    "print(coord_tokens[1])\n",
    "print(coordo_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be50bb5a-d7df-4ae8-93d7-17ba5afc2af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(coord_bin_values.reshape(1,-1) - coord.reshape(-1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148dea8-ee01-4f4d-90f3-9074533cdbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ab11ed06-e07d-42fd-96cc-7c6e20c5761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFMDataset(Dataset): \n",
    "    def __init__(self, data_path, tokenizer, transform, train_size=0.8, split='train'): \n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with h5py.File(self.data_path, 'r') as f:\n",
    "            total_length = f['x'].shape[0]\n",
    "            self.train_length = int(train_size*total_length)\n",
    "            self.val_length = total_length - self.train_length\n",
    "\n",
    "    def __len__(self): \n",
    "        if self.split == 'train': \n",
    "            return self.train_length\n",
    "        else: \n",
    "            return self.val_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.split == 'train': \n",
    "            idx += 0\n",
    "        else: \n",
    "            idx += self.train_length\n",
    "\n",
    "        with h5py.File(self.data_path, 'r') as f:\n",
    "            x = f['x'][idx]\n",
    "            xyz = f['xyz'][idx]\n",
    "\n",
    "        # Remove padding atoms\n",
    "        xyz = xyz[xyz[:, -1] > 0]\n",
    "\n",
    "        # Get edges \n",
    "        edges = []\n",
    "        for i in range(xyz.shape[0]): \n",
    "            for j in range(i+1, xyz.shape[0]): \n",
    "                dist = np.linalg.norm(xyz[i, :3] - xyz[j, :3])\n",
    "                if dist < 1.2*(COVALENT_RADII[xyz[i, -1]] + COVALENT_RADII[xyz[j, -1]]):\n",
    "                    edges.append([i,j])\n",
    "\n",
    "        # Normalize xyz to [0.25, 0.75]\n",
    "        xyzmin = np.min(xyz[:, :3])\n",
    "        xyz_max = np.max(xyz[:, 3])\n",
    "\n",
    "        xyz[:, :3] = (xyz[:, :3] - xyzmin)/(xyz_max - xyzmin)\n",
    "        xyz[:, :3] = 0.5*xyz[:, :3] + 0.25\n",
    "\n",
    "        # map atom types to integers (0,1, ..)\n",
    "        xyz[:, -1] = [ELEMENT_TO_INDEX[atom_type] for atom_type in xyz[:, -1]]\n",
    "\n",
    "        #sample = {'coords': xyz, edges: np.asarray(edges)}\n",
    "\n",
    "        #if self.transform:\n",
    "        #    sample = self.transform(sample)\n",
    "\n",
    "        # Keep all channels from HDF5 and convert to [C,H,W]\n",
    "        x = torch.from_numpy(x)\n",
    "        if x.dim() == 4 and x.size(0) == 1: \n",
    "            # Input like [1, H, W, C] -> [H, W, C]\n",
    "            x = x.squeeze(0)\n",
    "        if x.dim() == 3: \n",
    "            # Assume [H, W, C] from HDF5, move channels first\n",
    "            x = x.permute(2,0,1).contiguous()\n",
    "\n",
    "        mask = xyz[:, -1] > 0\n",
    "        atomtok = \" \".join([INDEX_TO_SYMBOL[value] for value in xyz[mask, -1]])\n",
    "\n",
    "        #atomtok = self.tokenizer['atomtok'].text_to_sequence(atomtok)\n",
    "\n",
    "        nodes = {'coords': [], 'symbols': \"\"}\n",
    "        for symbol, coord in zip(atomtok, xyz):\n",
    "            new_coord = coord_bin_values[np.argmin(np.abs(coord_bin_values.reshape(1,-1) - coord.reshape(-1,1)), axis = 1)]\n",
    "            #atomtok_coords.append(f'{symbol}: {new_coord[0]}, {new_coord[1]}, {new_coord[2]},')\n",
    "            nodes['coords'].append(new_coord[:2])\n",
    "            nodes['symbols'] = symbol + \" \"\n",
    "\n",
    "        atomtok_coords = self.tokenizer['atomtok_coords'].nodes_to_sequence(nodes)\n",
    "\n",
    "        #ref = {'atomtok': atomtok, 'edges': np.asarray(edges), 'atomtok_coords': atomtok_coords, 'chartok_coords': atomtok_coords}\n",
    "        ref = {'atomtok_coords': atomtok_coords}\n",
    "\n",
    "        return idx, x, ref\n",
    "\n",
    "\n",
    "def get_datasets(data_path, tokenizer, train_transform = None, val_transform = None, train_size=0.8):\n",
    "\n",
    "    train_dataset = AFMDataset(data_path, tokenizer = tokenizer, transform=train_transform, train_size=train_size, split='train')\n",
    "    val_dataset = AFMDataset(data_path, tokenizer = tokenizer, transform=val_transform, train_size=train_size, split='val')\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def afm_collate_fn(batch):\n",
    "\n",
    "    #sample = {'coords':[], 'edges':[]}\n",
    "    #ref = {'atomtok': [], 'edges': [], 'atomtok_coords': [], 'chartok_coords': []}\n",
    "    ref = {'atomtok_coords': []}\n",
    "\n",
    "    PAD_ID = 0\n",
    "    length = 128\n",
    "\n",
    "    ids = [id[0] for id in batch]\n",
    "    images = torch.stack([item[1] for item in batch])\n",
    "    for item in batch:\n",
    "        #sample['coords'].append(torch.from_numpy(item[2]['coords']))\n",
    "        #sample['edges'].append(torch.from_numpy(item[2]['edges']))\n",
    "        #ref['atomtok'].append(torch.from_numpy(item[2]['atomtok']))\n",
    "        #ref['edges'].append(torch.from_numpy(item[2]['edges']))\n",
    "\n",
    "        tok = item[2]['atomtok_coords']\n",
    "        \n",
    "        pad = [PAD_ID]*(length - len(tok))\n",
    "        tok.extend(pad)\n",
    "        \n",
    "        ref['atomtok_coords'] = tok\n",
    "        #ref['atomtok_coords'].append(torch.from_numpy(item[2]['atomtok_coords']))\n",
    "        #ref['chartok_coords'].append(torch.from_numpy(item[2]['chartok_coords']))\n",
    "\n",
    "    return ids, images, ref\n",
    "    #return ids, images, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c4ee79fc-24ec-4d91-8764-165a2b131cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(batch_size=32, learning_rate=0.001, encoder='swin_base', use_checkpoint=False, encoder_dim=64, in_chans=10, dec_hidden_size=16, enc_pos_emb=True, dec_num_layers=3, dec_attn_heads=4, hidden_dropout=0.2, attn_dropout=0.2, max_relative_positions=10, compute_confidence=True, formats=['atomtok_coords'], vocab_file='/home/sethih1/MORAFInator/src/vocab/vocab_chars.json', coord_bins=64, sep_xy=False, continuous_coords=True, input_resolution=256)\n"
     ]
    }
   ],
   "source": [
    "from src.tokenization import get_tokenizer\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args_dict = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    # related to model encoder\n",
    "    'encoder': 'swin_base', \n",
    "    'use_checkpoint':False, \n",
    "    'encoder_dim': 64, \n",
    "    'in_chans': 10,\n",
    "\n",
    "    # related to model decoder\n",
    "    'dec_hidden_size': 16,\n",
    "    'enc_pos_emb': True, \n",
    "    'dec_num_layers': 3, \n",
    "    'dec_attn_heads': 4,\n",
    "    'hidden_dropout': 0.2,\n",
    "    'attn_dropout': 0.2,\n",
    "    'max_relative_positions': 10,\n",
    "    'compute_confidence': True,\n",
    "\n",
    "    # related to tokenizer\n",
    "    'formats':['atomtok_coords'],\n",
    "    'vocab_file': '/home/sethih1/MORAFInator/src/vocab/vocab_chars.json', \n",
    "    'coord_bins': 64, \n",
    "    'sep_xy': False, \n",
    "    'continuous_coords': True, \n",
    "    'input_resolution': 256\n",
    "    \n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**args_dict)\n",
    "print(args)\n",
    "\n",
    "tokenizer = get_tokenizer(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ce3d5-c6f6-4872-85d0-7c047d7dd604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "92fa5698-2af6-4838-b05b-72fbff43fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path =  \"/scratch/phys/project/sin/hackathon/data/afm.h5\"\n",
    "train_dataset, val_dataset = get_datasets(data_path = h5_path, tokenizer=tokenizer, train_transform = None, val_transform = None, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "71c2b97c-06af-4c66-af62-7b4c7e0fddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=1, drop_last = False, collate_fn=afm_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "eb2f9d7c-c1a5-44b8-83f1-6262e4b39a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "tok = [1, 125, 143, 3, 145, 147, 3, 2]\n",
    "pad = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "print(tok.extend(pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c1c00c15-5a97-4d14-b6aa-e78206fc4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, imgs, refs = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fd8fda45-c4d1-4b9f-8893-1b3dcb9908d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "73623764-2748-49f6-8f37-715ec4e26b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 128, 128])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "343e8ed9-bb14-40cc-8da1-511447d44d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atomtok_coords': [1,\n",
       "  125,\n",
       "  143,\n",
       "  3,\n",
       "  145,\n",
       "  147,\n",
       "  3,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce838fa-3461-488a-acf7-f786b9bc8692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolNexTR_env",
   "language": "python",
   "name": "internal_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
