{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62315e7d-858d-42e2-a890-ad27141274e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source /scratch/phys/sin/sethih1/venv/MolNexTR_env/bin/activate\n",
    "import sys\n",
    "sys.path.append('/home/sethih1/MORAFInator/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c625705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import time \n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.optim import Adam, AdamW, SGD\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from src.components import Encoder, Decoder\n",
    "from src.loss_fuc import Criterion\n",
    "from src.utils import seed_torch, save_args, init_summary_writer, LossMeter, AverageMeter, asMinutes, timeSince, print_rank_0, format_df\n",
    "from src.chemical import convert_graph_to_smiles, postprocess_smiles, keep_main_molecule\n",
    "from src.tokenization import get_tokenizer\n",
    "from src.dataloader.data_loader import get_datasets, afm_collate_fn\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c8c11b2e-48a7-4f7b-9f6c-e6740d4b0553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(batch_size=32, learning_rate=0.001, encoder='swin_base', use_checkpoint=False, encoder_dim=64, in_chans=10, dec_hidden_size=16, enc_pos_emb=True, dec_num_layers=3, dec_attn_heads=4, hidden_dropout=0.2, attn_dropout=0.2, max_relative_positions=10, compute_confidence=True, formats=['atomtok_coords'], vocab_file='/home/sethih1/MORAFInator/src/vocab/vocab_chars.json', coord_bins=64, sep_xy=False, continuous_coords=True)\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args_dict = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    # related to model encoder\n",
    "    'encoder': 'swin_base', \n",
    "    'use_checkpoint':False, \n",
    "    'encoder_dim': 64, \n",
    "    'in_chans': 10,\n",
    "\n",
    "    # related to model decoder\n",
    "    'dec_hidden_size': 16,\n",
    "    'enc_pos_emb': True, \n",
    "    'dec_num_layers': 3, \n",
    "    'dec_attn_heads': 4,\n",
    "    'hidden_dropout': 0.2,\n",
    "    'attn_dropout': 0.2,\n",
    "    'max_relative_positions': 10,\n",
    "    'compute_confidence': True,\n",
    "\n",
    "    # related to tokenizer\n",
    "    'formats':['atomtok_coords'],\n",
    "    'vocab_file': '/home/sethih1/MORAFInator/src/vocab/vocab_chars.json', \n",
    "    'coord_bins': 64, \n",
    "    'sep_xy': False, \n",
    "    'continuous_coords': True\n",
    "    \n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**args_dict)\n",
    "print(args)\n",
    "\n",
    "tokenizer = get_tokenizer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9a5fa25-a49d-4554-83ae-80bcce21ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(args, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c084aa18-f4ee-41ae-892d-fff2c3fc67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim = encoder.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "36a13f1c-49f6-4894-9dee-68656eb87b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (decoder): ModuleDict(\n",
      "    (atomtok_coords): TransformerDecoderAR(\n",
      "      (enc_trans_layer): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=16, bias=True)\n",
      "      )\n",
      "      (enc_pos_emb): Embedding(144, 64)\n",
      "      (decoder): TransformerDecoder(\n",
      "        (layer_norm): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "        (transformer_layers): ModuleList(\n",
      "          (0-2): 3 x TransformerDecoderLayer(\n",
      "            (self_attn): MultiHeadedAttention(\n",
      "              (linear_keys): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_values): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_query): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "              (final_linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (relative_positions_embeddings): Embedding(21, 4)\n",
      "            )\n",
      "            (feed_forward): PositionwiseFeedForward(\n",
      "              (w_1): Linear(in_features=16, out_features=64, bias=True)\n",
      "              (w_2): Linear(in_features=64, out_features=16, bias=True)\n",
      "              (layer_norm): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "              (dropout_1): Dropout(p=0.2, inplace=False)\n",
      "              (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "            (layer_norm_1): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "            (context_attn): MultiHeadedAttention(\n",
      "              (linear_keys): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_values): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_query): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "              (final_linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "            (layer_norm_2): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (output_layer): Linear(in_features=16, out_features=165, bias=True)\n",
      "      (embeddings): Embeddings(\n",
      "        (make_embedding): Sequential(\n",
      "          (emb_luts): Elementwise(\n",
      "            (0): Embedding(165, 16, padding_idx=0)\n",
      "          )\n",
      "          (pe): PositionalEncoding(\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(args, tokenizer)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d60f5b31-e579-488a-a477-4822827091fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = AdamW(encoder.parameters(), lr = 0.001, weight_decay=0.0001)\n",
    "decoder_optimizer = AdamW(decoder.parameters(), lr= 0.001, weight_decay= 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f77eefcc-9b5c-4703-b35c-9d2d7fcf5bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f75af650-0829-49f2-b722-70cac453980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path =  \"/scratch/phys/project/sin/hackathon/data/afm.h5\"\n",
    "train_dataset, val_dataset = get_datasets(data_path = h5_path, train_transform = None, val_transform = None, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ae05c7b8-3f2f-40e9-92a1-2afdc95d181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=1, drop_last = False, collate_fn=afm_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "95d39da7-f328-4f4d-a402-32d8d852d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, imgs, samples = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5abdd9ee-8b80-463c-9396-e3e1cd316c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 3.1844e-01,  4.7720e-01,  5.0678e-01, -1.6296e-01,  4.0000e+00],\n",
       "         [ 4.9206e-01,  5.1817e-01,  5.0751e-01, -2.5634e-01,  3.0000e+00],\n",
       "         [ 4.3178e-01,  4.4547e-01,  5.0734e-01, -2.3490e-01,  2.0000e+00],\n",
       "         [ 7.0399e-01,  4.8344e-01,  5.0839e-01, -1.2060e-02,  1.0000e+00],\n",
       "         [ 3.1030e-01,  6.2991e-01,  5.0672e-01,  3.6500e-03,  1.0000e+00],\n",
       "         [ 6.7054e-01,  5.3211e-01,  5.0818e-01,  2.8600e-03,  1.0000e+00],\n",
       "         [ 6.7804e-01,  4.3030e-01,  5.0836e-01,  3.7000e-03,  1.0000e+00],\n",
       "         [ 2.9426e-01,  5.7316e-01,  5.0665e-01, -2.6030e-02,  1.0000e+00],\n",
       "         [ 6.1174e-01,  5.2794e-01,  5.0793e-01,  2.1310e-02,  1.0000e+00],\n",
       "         [ 6.1935e-01,  4.2582e-01,  5.0811e-01, -1.2160e-02,  1.0000e+00],\n",
       "         [ 3.6765e-01,  6.4423e-01,  5.0696e-01,  1.3800e-03,  1.0000e+00],\n",
       "         [ 3.3551e-01,  5.3128e-01,  5.0682e-01,  2.2484e-01,  1.0000e+00],\n",
       "         [ 4.8645e-01,  4.2538e-01,  5.0762e-01,  1.5270e-02,  1.0000e+00],\n",
       "         [ 4.3694e-01,  5.0046e-01,  5.0731e-01,  3.8458e-01,  1.0000e+00],\n",
       "         [ 3.9354e-01,  5.4426e-01,  5.0708e-01, -2.6210e-01,  1.0000e+00],\n",
       "         [ 5.8528e-01,  4.7466e-01,  5.0789e-01, -1.4868e-01,  1.0000e+00],\n",
       "         [ 5.2395e-01,  4.6961e-01,  5.0767e-01,  2.0025e-01,  1.0000e+00],\n",
       "         [ 4.0854e-01,  6.0192e-01,  5.0714e-01,  3.0600e-02,  1.0000e+00],\n",
       "         [ 6.9038e-01,  5.7378e-01,  5.0820e-01,  2.0340e-02,  0.0000e+00],\n",
       "         [ 7.0378e-01,  3.9200e-01,  5.0853e-01,  2.0260e-02,  0.0000e+00],\n",
       "         [ 3.8055e-01,  6.8850e-01,  5.0702e-01,  2.2570e-02,  0.0000e+00],\n",
       "         [ 2.5000e-01,  5.6031e-01,  5.0648e-01,  3.4840e-02,  0.0000e+00],\n",
       "         [ 4.5324e-01,  6.1297e-01,  5.0734e-01,  2.6450e-02,  0.0000e+00],\n",
       "         [ 5.9970e-01,  3.8410e-01,  5.0810e-01,  1.1260e-02,  0.0000e+00],\n",
       "         [ 4.9530e-01,  3.8033e-01,  5.0774e-01,  2.9460e-02,  0.0000e+00],\n",
       "         [ 2.7797e-01,  6.6281e-01,  5.0658e-01,  2.7760e-02,  0.0000e+00],\n",
       "         [ 5.8585e-01,  5.6604e-01,  5.0778e-01,  1.4540e-02,  0.0000e+00],\n",
       "         [ 7.5000e-01,  4.8682e-01,  5.0858e-01,  1.9320e-02,  0.0000e+00]])]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['coords']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "21a309ce-5703-4426-9b5b-03f237ac2ed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[43mrefs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m     20\u001b[0m features, hiddens \u001b[38;5;241m=\u001b[39m encoder(imgs, refs)\n\u001b[1;32m     21\u001b[0m results \u001b[38;5;241m=\u001b[39m decoder(features, hiddens, refs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for step, (indices, imgs, refs) in enumerate(train_loader):\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        refs = refs.to(device)\n",
    "\n",
    "\n",
    "        features, hiddens = encoder(imgs, refs)\n",
    "        results = decoder(features, hiddens, refs)\n",
    "        losses = criterion(results, refs)\n",
    "        loss = sum(losses.value())\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        losses.append(loss.value())\n",
    "\n",
    "    train_loss = torch.mean(torch.tensor(losses))\n",
    "\n",
    "    print(f\"Epoch {epoch} training loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93914dfd-b10d-457a-b8a6-c48ccbf5c76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d8350b0b-75ba-48a6-a2e2-12e279b79b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NEW: map element indices (from AFMData) to atomic symbols used by tokenizer\n",
    "INDEX_TO_SYMBOL = {\n",
    "    0: 'H',\n",
    "    1: 'C',\n",
    "    2: 'N',\n",
    "    3: 'O',\n",
    "    4: 'F',\n",
    "}\n",
    "\n",
    "class AFM3DToChartokConverter:\n",
    "    \"\"\"3D AFM to ChartTok converter for training\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, coord_bins=64, default_atom='C', use_3d=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.coord_bins = coord_bins\n",
    "        self.default_atom = default_atom\n",
    "        self.use_3d = use_3d\n",
    "        \n",
    "        # Create coordinate tokens (bins for X, Y, Z)\n",
    "        self.coord_tokens = [f'<COORD_{i}>' for i in range(coord_bins)]\n",
    "    \n",
    "    def coords_3d_to_token_sequence(self, coords_3d, symbols):\n",
    "        \"\"\"Convert 3D coordinates and symbols to token sequence (symbols + SOS/EOS).\"\"\"\n",
    "        if coords_3d is None or len(coords_3d) == 0 or len(symbols) == 0:\n",
    "            return torch.tensor([1, 2], dtype=torch.long)  # SOS, EOS\n",
    "        \n",
    "        sequence = [1]  # SOS token\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            if symbol in self.tokenizer.stoi:\n",
    "                sequence.append(self.tokenizer.stoi[symbol])\n",
    "            else:\n",
    "                sequence.append(self.tokenizer.stoi.get(self.default_atom, UNK_ID))\n",
    "        \n",
    "        sequence.append(2)  # EOS\n",
    "        return torch.tensor(sequence, dtype=torch.long)\n",
    "    \n",
    "    def _normalize_coords_3d(self, coords):\n",
    "        \"\"\"Normalize 3D coordinates to [0, 1] range\"\"\"\n",
    "        coords = np.array(coords)\n",
    "        if len(coords) == 0:\n",
    "            return coords\n",
    "        \n",
    "        # Per-dimension normalization\n",
    "        coords_norm = np.zeros_like(coords)\n",
    "        for dim in range(3):\n",
    "            coord_dim = coords[:, dim]\n",
    "            min_val, max_val = coord_dim.min(), coord_dim.max()\n",
    "            if max_val > min_val:\n",
    "                coords_norm[:, dim] = (coord_dim - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                coords_norm[:, dim] = 0.5  # Default to middle if all same\n",
    "        \n",
    "        return coords_norm\n",
    "    \n",
    "    def convert_single_molecule_3d(self, nodes, edges=None):\n",
    "        \"\"\"Convert single AFM molecule to 3D chartok format\"\"\"\n",
    "        if hasattr(nodes, 'x') and hasattr(nodes, 'pos'):\n",
    "            # PyTorch Geometric format\n",
    "            symbols = [self.default_atom] * len(nodes.x)  # Simplified - use default atom\n",
    "            coords_3d = nodes.pos.numpy() if hasattr(nodes.pos, 'numpy') else nodes.pos\n",
    "        elif isinstance(nodes, dict):\n",
    "            # Dictionary format\n",
    "            if 'pos' in nodes and 'x' in nodes:\n",
    "                coords_3d = nodes['pos']\n",
    "                symbols = [self.default_atom] * len(coords_3d)\n",
    "            else:\n",
    "                # Default fallback\n",
    "                coords_3d = np.random.rand(5, 3)  # Random 3D positions\n",
    "                symbols = [self.default_atom] * 5\n",
    "        else:\n",
    "            # Tensor or array format\n",
    "            if hasattr(nodes, 'shape') and len(nodes.shape) >= 2:\n",
    "                if nodes.shape[-1] >= 3:\n",
    "                    coords_3d = nodes[:, :3] if len(nodes.shape) == 2 else nodes.reshape(-1, 3)[:, :3]\n",
    "                    symbols = [self.default_atom] * len(coords_3d)\n",
    "                else:\n",
    "                    # Fallback to random\n",
    "                    coords_3d = np.random.rand(5, 3)\n",
    "                    symbols = [self.default_atom] * 5\n",
    "            else:\n",
    "                coords_3d = np.random.rand(5, 3)\n",
    "                symbols = [self.default_atom] * 5\n",
    "        \n",
    "        # Convert to token sequence\n",
    "        token_sequence = self.coords_3d_to_token_sequence(coords_3d, symbols)\n",
    "        \n",
    "        # Create info dict\n",
    "        nodes_3d_dict = {\n",
    "            'coords_3d': coords_3d.tolist() if hasattr(coords_3d, 'tolist') else coords_3d,\n",
    "            'symbols': symbols,\n",
    "            'num_atoms': len(symbols)\n",
    "        }\n",
    "        \n",
    "        return token_sequence, nodes_3d_dict\n",
    "\n",
    "# NEW: Dataset wrapper that uses AFMData (HDF5) and converts to model-friendly items\n",
    "class H5AFM3DDataset(Dataset):\n",
    "    def __init__(self, h5_path, tokenizer, coord_bins=64):\n",
    "        super().__init__()\n",
    "        self.afm = AFMData(h5_path, transform=None, train_size=1.0, split='train')\n",
    "        self.converter = AFM3DToChartokConverter(tokenizer, coord_bins=coord_bins, default_atom='C', use_3d=True)\n",
    "    def __len__(self):\n",
    "        return len(self.afm)\n",
    "    def __getitem__(self, idx):\n",
    "        idx_out, image, sample = self.afm[idx]\n",
    "        # sample['coords']: numpy array [num_atoms, 5] -> x,y,z,charge?,element_index(0..4)\n",
    "        coords_np = sample['coords']\n",
    "        edges_np = sample['edges']  # (E, 2)\n",
    "        coords_3d = torch.from_numpy(coords_np[:, :3]).float() if coords_np.size > 0 else torch.zeros((0, 3), dtype=torch.float32)\n",
    "        # derive symbols from last column (already mapped to 0..4)\n",
    "        elem_idx = coords_np[:, -1].astype(int) if coords_np.size > 0 else np.array([], dtype=int)\n",
    "        symbols = [INDEX_TO_SYMBOL.get(int(i), 'C') for i in elem_idx]\n",
    "        # build token sequence for chartok_coords\n",
    "        sequence = self.converter.coords_3d_to_token_sequence(coords_3d.numpy(), symbols)\n",
    "        return {\n",
    "            'idx': torch.tensor(idx_out, dtype=torch.long),\n",
    "            'image': image,                         # tensor [3,H,W]\n",
    "            'sequence': sequence,                   # tensor [T]\n",
    "            'seq_length': torch.tensor(len(sequence), dtype=torch.long),\n",
    "            'coords_3d': coords_3d,                 # tensor [N,3]\n",
    "            'symbols': symbols,\n",
    "            'num_atoms': torch.tensor(len(symbols), dtype=torch.long),\n",
    "            'edges_list': torch.from_numpy(edges_np).long() if edges_np.size > 0 else torch.zeros((0, 2), dtype=torch.long),\n",
    "        }\n",
    "\n",
    "class Complete3DAFMDataset:\n",
    "    \"\"\"Complete 3D AFM dataset wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, afm_dataset, tokenizer, coord_bins=64, default_atom='C', use_3d=True):\n",
    "        self.afm_dataset = afm_dataset\n",
    "        self.converter = AFM3DToChartokConverter(tokenizer, coord_bins, default_atom, use_3d)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.afm_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        # Get AFM sample\n",
    "        afm_sample = self.afm_dataset[idx]\n",
    "        \n",
    "        # Extract molecular data\n",
    "        nodes = afm_sample.get('nodes', afm_sample.get('x', None))\n",
    "        edges = afm_sample.get('edges', None)\n",
    "        \n",
    "        if nodes is not None:\n",
    "            chartok_3d_seq, nodes_3d_dict = self.converter.convert_single_molecule_3d(nodes, edges)\n",
    "            \n",
    "            converted_sample = {\n",
    "                'idx': idx,\n",
    "                'chartok_coords_3d': chartok_3d_seq,\n",
    "                'nodes_3d_dict': nodes_3d_dict,\n",
    "                'original_afm_sample': afm_sample,\n",
    "                'coords_3d': torch.tensor(nodes_3d_dict['coords_3d'], dtype=torch.float32),\n",
    "                'symbols': nodes_3d_dict['symbols']\n",
    "            }\n",
    "        else:\n",
    "            # Fallback\n",
    "            converted_sample = {\n",
    "                'idx': idx,\n",
    "                'chartok_coords_3d': torch.tensor([1, 2]),\n",
    "                'nodes_3d_dict': {'coords_3d': [], 'symbols': []},\n",
    "                'original_afm_sample': afm_sample,\n",
    "                'coords_3d': torch.tensor([]),\n",
    "                'symbols': []\n",
    "            }\n",
    "        \n",
    "        self.cache[idx] = converted_sample\n",
    "        return converted_sample\n",
    "\n",
    "class AFM3DCollator:\n",
    "    \"\"\"Custom collate function for 3D AFM data\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        \n",
    "        sequences = [sample['chartok_coords_3d'] for sample in batch]\n",
    "        coords_3d_list = [sample['coords_3d'] for sample in batch]\n",
    "        symbols_list = [sample['symbols'] for sample in batch]\n",
    "        \n",
    "        # Pad sequences\n",
    "        max_seq_len = min(max(len(seq) for seq in sequences), self.max_length)\n",
    "        padded_sequences = torch.zeros((batch_size, max_seq_len), dtype=torch.long)\n",
    "        attention_masks = torch.zeros((batch_size, max_seq_len), dtype=torch.bool)\n",
    "        \n",
    "        # 3D coordinates\n",
    "        max_atoms = max(len(coords) for coords in coords_3d_list) if coords_3d_list[0].numel() > 0 else 1\n",
    "        coords_3d_batch = torch.zeros((batch_size, max_atoms, 3))\n",
    "        coords_mask = torch.zeros((batch_size, max_atoms), dtype=torch.bool)\n",
    "        \n",
    "        for i, (seq, coords_3d, symbols) in enumerate(zip(sequences, coords_3d_list, symbols_list)):\n",
    "            seq_len = min(len(seq), max_seq_len)\n",
    "            padded_sequences[i, :seq_len] = seq[:seq_len]\n",
    "            attention_masks[i, :seq_len] = True\n",
    "            \n",
    "            if coords_3d.numel() > 0 and len(coords_3d.shape) == 2:\n",
    "                atom_len = min(len(coords_3d), max_atoms)\n",
    "                coords_3d_batch[i, :atom_len] = coords_3d[:atom_len]\n",
    "                coords_mask[i, :atom_len] = True\n",
    "        \n",
    "        return {\n",
    "            'input_ids': padded_sequences,\n",
    "            'attention_mask': attention_masks,\n",
    "            'coords_3d': coords_3d_batch,\n",
    "            'coords_mask': coords_mask,\n",
    "            'symbols_batch': symbols_list,\n",
    "            'batch_size': batch_size,\n",
    "            'max_seq_len': max_seq_len,\n",
    "            'max_atoms': max_atoms\n",
    "        }\n",
    "\n",
    "def make_collate_fn(tokenizer):\n",
    "    \"\"\"Create a collate function that maps out-of-range tokens to UNK and batches coords/edges.\"\"\"\n",
    "    vocab_size = len(tokenizer)\n",
    "    base_vocab = len(tokenizer.stoi)\n",
    "\n",
    "    def _collate(batch):\n",
    "        # indices and images\n",
    "        indices = torch.stack([item['idx'] for item in batch])\n",
    "        images = torch.stack([item['image'] for item in batch])\n",
    "\n",
    "        # sequences with UNK clamping\n",
    "        raw_sequences = [item['sequence'] for item in batch]\n",
    "        sequences = []\n",
    "        seq_lengths = []\n",
    "        for seq in raw_sequences:\n",
    "            seq = seq.clone()\n",
    "            invalid = (seq >= vocab_size) | (seq < 0)\n",
    "            if invalid.any():\n",
    "                seq[invalid] = UNK_ID\n",
    "            sequences.append(seq)\n",
    "            seq_lengths.append(torch.tensor(len(seq), dtype=torch.long))\n",
    "        seq_lengths = torch.stack(seq_lengths)\n",
    "\n",
    "        # debug token range for first few batches\n",
    "        all_tokens = torch.cat(sequences) if len(sequences) > 0 else torch.tensor([], dtype=torch.long)\n",
    "        if all_tokens.numel() > 0:\n",
    "            max_token = all_tokens.max().item(); min_token = all_tokens.min().item()\n",
    "        else:\n",
    "            max_token = min_token = 0\n",
    "        if not hasattr(_collate, 'call_count'):\n",
    "            _collate.call_count = 0\n",
    "        _collate.call_count += 1\n",
    "        if _collate.call_count <= 3:\n",
    "            logger.info(f\"ðŸ” Batch {_collate.call_count}: Token range [{min_token}, {max_token}], base_vocab={base_vocab}, total_vocab={vocab_size}, sequences={len(sequences)}\")\n",
    "\n",
    "        # pad sequences\n",
    "        max_len = max(len(seq) for seq in sequences) if sequences else 0\n",
    "        batch_size = len(batch)\n",
    "        padded_sequences = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            L = len(seq)\n",
    "            if L > 0:\n",
    "                padded_sequences[i, :L] = seq\n",
    "\n",
    "        # coords_3d batching (pad to max atoms), use -100 for masked positions\n",
    "        coords_list = [item['coords_3d'] for item in batch]\n",
    "        num_atoms_list = [int(item['num_atoms']) for item in batch]\n",
    "        max_atoms = max(num_atoms_list) if num_atoms_list else 0\n",
    "        coords_3d_batch = torch.full((batch_size, max_atoms, 3), -100.0, dtype=torch.float32)\n",
    "        for i, coords in enumerate(coords_list):\n",
    "            n = min(coords.size(0), max_atoms)\n",
    "            if n > 0:\n",
    "                coords_3d_batch[i, :n, :] = coords[:n, :]\n",
    "\n",
    "        # atom_indices: positions of symbol tokens in sequence: [1..num_atoms]\n",
    "        atom_indices = torch.zeros(batch_size, max_atoms, dtype=torch.long)\n",
    "        for i, n_atoms in enumerate(num_atoms_list):\n",
    "            n = min(n_atoms, max_atoms)\n",
    "            if n > 0:\n",
    "                atom_indices[i, :n] = torch.arange(1, 1 + n)\n",
    "\n",
    "        # edges target placeholder: ignore all pairs by default (-100), shape [B, max_atoms, max_atoms]\n",
    "        edges_target = torch.full((batch_size, max_atoms, max_atoms), -100, dtype=torch.long)\n",
    "\n",
    "        refs = {\n",
    "            'chartok_coords': (padded_sequences, seq_lengths),\n",
    "            'coords_3d': coords_3d_batch,\n",
    "            'edges': edges_target,\n",
    "            'atom_indices': (atom_indices,),\n",
    "        }\n",
    "        return indices, images, refs\n",
    "\n",
    "    return _collate\n",
    "\n",
    "def create_3d_dataloader(data_path, batch_size=16, num_workers=4):\n",
    "    \"\"\"Create DataLoader for 3D AFM training from HDF5 using the provided pipeline\"\"\"\n",
    "    args = create_model_args()\n",
    "\n",
    "    # Determine HDF5 file path\n",
    "    h5_path = data_path\n",
    "    if os.path.isdir(data_path):\n",
    "        # pick the first .h5 file in the directory\n",
    "        h5_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.h5') or f.endswith('.hdf5')]\n",
    "        if len(h5_files) == 0:\n",
    "            raise FileNotFoundError(f\"No .h5/.hdf5 files found under {data_path}\")\n",
    "        h5_path = sorted(h5_files)[0]\n",
    "\n",
    "    # Build tokenizer(s)\n",
    "    tokenizers = get_tokenizer(args)\n",
    "    chartok_tok = tokenizers['chartok_coords']\n",
    "    logger.info(f\"ðŸ“š Tokenizer built: chartok_coords | base_vocab={len(chartok_tok.stoi)} | total_vocab={len(chartok_tok)} | coord_bins={args.coord_bins}\")\n",
    "\n",
    "    # Create dataset from HDF5\n",
    "    dataset = H5AFM3DDataset(h5_path, chartok_tok, coord_bins=args.coord_bins)\n",
    "    logger.info(f\"ðŸ“‚ Using AFM HDF5 dataset: {h5_path} | {len(dataset)} samples\")\n",
    "\n",
    "    # Collate function\n",
    "    collate_fn = make_collate_fn(chartok_tok)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    logger.info(f\"âœ… DataLoader created with {len(dataset)} samples, {len(train_loader)} batches\")\n",
    "    return train_loader, tokenizers, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "13073b14-4cd3-4b7c-8527-28bbc66ac820",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_model_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_3d_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[109], line 295\u001b[0m, in \u001b[0;36mcreate_3d_dataloader\u001b[0;34m(data_path, batch_size, num_workers)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_3d_dataloader\u001b[39m(data_path, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m    294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create DataLoader for 3D AFM training from HDF5 using the provided pipeline\"\"\"\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model_args\u001b[49m()\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# Determine HDF5 file path\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     h5_path \u001b[38;5;241m=\u001b[39m data_path\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_model_args' is not defined"
     ]
    }
   ],
   "source": [
    "data = create_3d_dataloader(h5_path, batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d345fcd5-daaa-4522-9b0c-9bd765a4fb0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[0;32mIn[109], line 146\u001b[0m, in \u001b[0;36mComplete3DAFMDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    143\u001b[0m afm_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafm_dataset[idx]\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Extract molecular data\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[43mafm_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m'\u001b[39m, afm_sample\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    147\u001b[0m edges \u001b[38;5;241m=\u001b[39m afm_sample\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7227fa-45af-4f68-b6f3-7ea976ee04f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
