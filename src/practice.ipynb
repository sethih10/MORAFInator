{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62315e7d-858d-42e2-a890-ad27141274e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source /scratch/phys/sin/sethih1/venv/MolNexTR_env/bin/activate\n",
    "import sys\n",
    "sys.path.append('/home/sethih1/MORAFInator/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c625705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/phys/sin/sethih1/venv/MolNexTR_env/lib64/python3.9/site-packages/onmt/modules/sparse_activations.py:48: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, dim=0):\n",
      "/scratch/phys/sin/sethih1/venv/MolNexTR_env/lib64/python3.9/site-packages/onmt/modules/sparse_activations.py:68: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/scratch/phys/sin/sethih1/venv/MolNexTR_env/lib64/python3.9/site-packages/onmt/modules/sparse_losses.py:13: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, target):\n",
      "/scratch/phys/sin/sethih1/venv/MolNexTR_env/lib64/python3.9/site-packages/onmt/modules/sparse_losses.py:37: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import time \n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.optim import Adam, AdamW, SGD\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from src.components import Encoder, Decoder\n",
    "from src.loss_fuc import Criterion\n",
    "from src.utils import seed_torch, save_args, init_summary_writer, LossMeter, AverageMeter, asMinutes, timeSince, print_rank_0, format_df\n",
    "from src.chemical import convert_graph_to_smiles, postprocess_smiles, keep_main_molecule\n",
    "from src.tokenization import get_tokenizer\n",
    "from src.dataloader.data_loader import get_datasets, afm_collate_fn\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403f5a58-22f3-4f3a-953e-b48a3d54bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.02 0.03 0.05 0.06 0.08 0.1  0.11 0.13 0.14 0.16 0.17 0.19 0.21\n",
      " 0.22 0.24 0.25 0.27 0.29 0.3  0.32 0.33 0.35 0.37 0.38 0.4  0.41 0.43\n",
      " 0.44 0.46 0.48 0.49 0.51 0.52 0.54 0.56 0.57 0.59 0.6  0.62 0.63 0.65\n",
      " 0.67 0.68 0.7  0.71 0.73 0.75 0.76 0.78 0.79 0.81 0.83 0.84 0.86 0.87\n",
      " 0.89 0.9  0.92 0.94 0.95 0.97 0.98 1.  ]\n"
     ]
    }
   ],
   "source": [
    "coord_bins = 64\n",
    "\n",
    "coord_bin_values = np.round(np.linspace(0,1,64),2)\n",
    "print(coord_bin_values)\n",
    "coord_tokens = [f'<COORD_{i}>' for i in range(coord_bins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c11b2e-48a7-4f7b-9f6c-e6740d4b0553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(batch_size=32, learning_rate=0.001, encoder='swin_base', use_checkpoint=False, encoder_dim=1024, in_chans=10, dec_hidden_size=16, enc_pos_emb=True, dec_num_layers=3, dec_attn_heads=4, hidden_dropout=0.2, attn_dropout=0.2, max_relative_positions=10, compute_confidence=True, formats=['atomtok_coords', 'edges'], vocab_file='/home/sethih1/MORAFInator/src/vocab/vocab_chars.json', coord_bins=64, sep_xy=False, continuous_coords=True, input_resolution=256, label_smoothing=0.1)\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args_dict = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    # related to model encoder\n",
    "    'encoder': 'swin_base', \n",
    "    'use_checkpoint':False, \n",
    "    'encoder_dim': 1024, \n",
    "    'in_chans': 10,\n",
    "\n",
    "    # related to model decoder\n",
    "    'dec_hidden_size': 16,\n",
    "    'enc_pos_emb': True, \n",
    "    'dec_num_layers': 3, \n",
    "    'dec_attn_heads': 4,\n",
    "    'hidden_dropout': 0.2,\n",
    "    'attn_dropout': 0.2,\n",
    "    'max_relative_positions': 10,\n",
    "    'compute_confidence': True,\n",
    "\n",
    "    # related to tokenizer\n",
    "    'formats':['atomtok_coords', 'edges'],\n",
    "    'vocab_file': '/home/sethih1/MORAFInator/src/vocab/vocab_chars.json', \n",
    "    'coord_bins': 64, \n",
    "    'sep_xy': False, \n",
    "    'continuous_coords': True, \n",
    "    'input_resolution': 256,\n",
    "\n",
    "    # loss function\n",
    "    'label_smoothing': 0.1\n",
    "    \n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**args_dict)\n",
    "print(args)\n",
    "\n",
    "tokenizer = get_tokenizer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df2a4c82-b584-48ce-9540-c5c0bea29352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a5fa25-a49d-4554-83ae-80bcce21ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(args, pretrained=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9f24fb-9702-451e-811b-bdb9915ad239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "#summary(encoder, input_size=(1, 10, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c084aa18-f4ee-41ae-892d-fff2c3fc67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim = encoder.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a13f1c-49f6-4894-9dee-68656eb87b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (decoder): ModuleDict(\n",
      "    (atomtok_coords): TransformerDecoderAR(\n",
      "      (enc_trans_layer): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=16, bias=True)\n",
      "      )\n",
      "      (enc_pos_emb): Embedding(144, 1024)\n",
      "      (decoder): TransformerDecoder(\n",
      "        (layer_norm): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "        (transformer_layers): ModuleList(\n",
      "          (0-2): 3 x TransformerDecoderLayer(\n",
      "            (self_attn): MultiHeadedAttention(\n",
      "              (linear_keys): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_values): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_query): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "              (final_linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (relative_positions_embeddings): Embedding(21, 4)\n",
      "            )\n",
      "            (feed_forward): PositionwiseFeedForward(\n",
      "              (w_1): Linear(in_features=16, out_features=64, bias=True)\n",
      "              (w_2): Linear(in_features=64, out_features=16, bias=True)\n",
      "              (layer_norm): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "              (dropout_1): Dropout(p=0.2, inplace=False)\n",
      "              (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "            (layer_norm_1): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "            (context_attn): MultiHeadedAttention(\n",
      "              (linear_keys): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_values): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_query): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "              (final_linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "            (layer_norm_2): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (output_layer): Linear(in_features=16, out_features=165, bias=True)\n",
      "      (embeddings): Embeddings(\n",
      "        (make_embedding): Sequential(\n",
      "          (emb_luts): Elementwise(\n",
      "            (0): Embedding(165, 16, padding_idx=0)\n",
      "          )\n",
      "          (pe): PositionalEncoding(\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (edges): GraphPredictor(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=16, out_features=7, bias=True)\n",
      "      )\n",
      "      (coords_mlp): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=16, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(args, tokenizer)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d60f5b31-e579-488a-a477-4822827091fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = AdamW(encoder.parameters(), lr = 0.001, weight_decay=0.0001)\n",
    "decoder_optimizer = AdamW(decoder.parameters(), lr= 0.001, weight_decay= 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f75af650-0829-49f2-b722-70cac453980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path =  \"/scratch/phys/project/sin/hackathon/data/afm.h5\"\n",
    "train_dataset, val_dataset = get_datasets(data_path = h5_path, tokenizer=tokenizer, train_transform = None, val_transform = None, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae05c7b8-3f2f-40e9-92a1-2afdc95d181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=2, drop_last = False, collate_fn=afm_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95d39da7-f328-4f4d-a402-32d8d852d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, imgs, samples = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f03395d-18a9-4242-8b6d-0f654cb03d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['edges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a309ce-5703-4426-9b5b-03f237ac2ed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [1, 128]] is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(losses_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     45\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     48\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/scratch/phys/sin/sethih1/venv/MolNexTR_env/lib64/python3.9/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/phys/sin/sethih1/venv/MolNexTR_env/lib64/python3.9/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/phys/sin/sethih1/venv/MolNexTR_env/lib64/python3.9/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [1, 128]] is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "def to_device(obj, device):\n",
    "    \"\"\"Recursively move tensors inside lists/tuples/dicts to device.\"\"\"\n",
    "    if torch.is_tensor(obj):\n",
    "        return obj.to(device)\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_device(v, device) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [to_device(x, device) for x in obj]\n",
    "    if isinstance(obj, tuple):\n",
    "        return tuple(to_device(x, device) for x in obj)\n",
    "    return obj\n",
    "    \n",
    "\n",
    "epochs = 10\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "criterion = Criterion(args, tokenizer).to(device)\n",
    "for epoch in range(epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for step, (indices, imgs, refs) in enumerate(train_loader):\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        #refs = refs.to(device)\n",
    "        refs = to_device(refs, device)\n",
    "\n",
    "\n",
    "        features, hiddens = encoder(imgs)\n",
    "\n",
    "        \n",
    "        \n",
    "        #features, hiddens = encoder(imgs, refs)\n",
    "        results = decoder(features, hiddens, refs)\n",
    "        losses_dict = criterion(results, refs)\n",
    "        loss = sum(losses_dict.values())\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        losses.append(loss.value())\n",
    "\n",
    "    train_loss = torch.mean(torch.tensor(losses))\n",
    "\n",
    "    print(f\"Epoch {epoch} training loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93914dfd-b10d-457a-b8a6-c48ccbf5c76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8350b0b-75ba-48a6-a2e2-12e279b79b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13073b14-4cd3-4b7c-8527-28bbc66ac820",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_3d_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_3d_dataloader\u001b[49m(h5_path, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_3d_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "data = create_3d_dataloader(h5_path, batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345fcd5-daaa-4522-9b0c-9bd765a4fb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7227fa-45af-4f68-b6f3-7ea976ee04f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    def getitem(self, idx):\n",
    "        ref = {}\n",
    "        if self.dynamic_indigo:\n",
    "            begin = time.time()\n",
    "            image, smiles, graph, success = generate_indigo_image(\n",
    "                self.smiles[idx], mol_augment=self.args.mol_augment, default_option=self.args.default_option,\n",
    "                shuffle_nodes=self.args.shuffle_nodes, pseudo_coords=self.pseudo_coords,\n",
    "                include_condensed=self.args.include_condensed)\n",
    "            # raw_image = image\n",
    "            end = time.time()\n",
    "            if idx < 30 and self.args.save_image:\n",
    "                path = os.path.join(self.args.save_path, 'images')\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                cv2.imwrite(os.path.join(path, f'{idx}.png'), image)\n",
    "            if not success:\n",
    "                return idx, None, {}\n",
    "            image, coords = self.image_transform(image, graph['coords'], renormalize=self.pseudo_coords)\n",
    "            graph['coords'] = coords\n",
    "            ref['time'] = end - begin\n",
    "            if 'atomtok' in self.formats:\n",
    "                max_len = FORMAT_INFO['atomtok']['max_len']\n",
    "                label = self.tokenizer['atomtok'].text_to_sequence(smiles, tokenized=False)\n",
    "                ref['atomtok'] = torch.LongTensor(label[:max_len])\n",
    "            if 'edges' in self.formats and 'atomtok_coords' not in self.formats and 'chartok_coords' not in self.formats:\n",
    "                ref['edges'] = torch.tensor(graph['edges'])\n",
    "            if 'atomtok_coords' in self.formats:\n",
    "                self._process_atomtok_coords(idx, ref, smiles, graph['coords'], graph['edges'],\n",
    "                                             mask_ratio=self.args.mask_ratio)\n",
    "            if 'chartok_coords' in self.formats:\n",
    "                self._process_chartok_coords(idx, ref, smiles, graph['coords'], graph['edges'],\n",
    "                                             mask_ratio=self.args.mask_ratio)\n",
    "            return idx, image, ref\n",
    "        else:\n",
    "            file_path = self.file_paths[idx]\n",
    "            image = cv2.imread(file_path)\n",
    "            if image is None:\n",
    "                image = np.array([[[255., 255., 255.]] * 10] * 10).astype(np.float32)\n",
    "                print(file_path, 'not found!')\n",
    "            if self.coords_df is not None:\n",
    "                h, w, _ = image.shape\n",
    "                coords = np.array(eval(self.coords_df.loc[idx, 'node_coords']))\n",
    "                if self.pseudo_coords:\n",
    "                    coords = normalize_nodes(coords)\n",
    "                coords[:, 0] = coords[:, 0] * w\n",
    "                coords[:, 1] = coords[:, 1] * h\n",
    "                image, coords = self.image_transform(image, coords, renormalize=self.pseudo_coords)\n",
    "            else:\n",
    "                image = self.image_transform(image)\n",
    "                coords = None\n",
    "            if self.labelled:\n",
    "                smiles = self.smiles[idx]\n",
    "                if 'atomtok' in self.formats:\n",
    "                    max_len = FORMAT_INFO['atomtok']['max_len']\n",
    "                    label = self.tokenizer['atomtok'].text_to_sequence(smiles, False)\n",
    "                    ref['atomtok'] = torch.LongTensor(label[:max_len])\n",
    "                if 'atomtok_coords' in self.formats:\n",
    "                    if coords is not None:\n",
    "                        self._process_atomtok_coords(idx, ref, smiles, coords, mask_ratio=0)\n",
    "                    else:\n",
    "                        self._process_atomtok_coords(idx, ref, smiles, mask_ratio=1)\n",
    "                if 'chartok_coords' in self.formats:\n",
    "                    if coords is not None:\n",
    "                        self._process_chartok_coords(idx, ref, smiles, coords, mask_ratio=0)\n",
    "                    else:\n",
    "                        self._process_chartok_coords(idx, ref, smiles, mask_ratio=1)\n",
    "            if self.args.predict_coords and ('atomtok_coords' in self.formats or 'chartok_coords' in self.formats):\n",
    "                smiles = self.smiles[idx]\n",
    "                if 'atomtok_coords' in self.formats:\n",
    "                    self._process_atomtok_coords(idx, ref, smiles, mask_ratio=1)\n",
    "                if 'chartok_coords' in self.formats:\n",
    "                    self._process_chartok_coords(idx, ref, smiles, mask_ratio=1)\n",
    "            return idx, image, ref\n",
    "\n",
    "    def _process_atomtok_coords(self, idx, ref, smiles, coords=None, edges=None, mask_ratio=0):\n",
    "        max_len = FORMAT_INFO['atomtok_coords']['max_len']\n",
    "        tokenizer = self.tokenizer['atomtok_coords']\n",
    "        if smiles is None or type(smiles) is not str:\n",
    "            smiles = \"\"\n",
    "        label, indices = tokenizer.smiles_to_sequence(smiles, coords, mask_ratio=mask_ratio)\n",
    "        ref['atomtok_coords'] = torch.LongTensor(label[:max_len])\n",
    "        indices = [i for i in indices if i < max_len]\n",
    "        ref['atom_indices'] = torch.LongTensor(indices)\n",
    "        if tokenizer.continuous_coords:\n",
    "            if coords is not None:\n",
    "                ref['coords'] = torch.tensor(coords)\n",
    "            else:\n",
    "                ref['coords'] = torch.ones(len(indices), 2) * -1.\n",
    "        if edges is not None:\n",
    "            ref['edges'] = torch.tensor(edges)[:len(indices), :len(indices)]\n",
    "        else:\n",
    "            if 'edges' in self.df.columns:\n",
    "                edge_list = eval(self.df.loc[idx, 'edges'])\n",
    "                n = len(indices)\n",
    "                edges = torch.zeros((n, n), dtype=torch.long)\n",
    "                for u, v, t in edge_list:\n",
    "                    if u < n and v < n:\n",
    "                        if t <= 4:\n",
    "                            edges[u, v] = t\n",
    "                            edges[v, u] = t\n",
    "                        else:\n",
    "                            edges[u, v] = t\n",
    "                            edges[v, u] = 11 - t\n",
    "                ref['edges'] = edges\n",
    "            else:\n",
    "                ref['edges'] = torch.ones(len(indices), len(indices), dtype=torch.long) * (-100)\n",
    "\n",
    "    def _process_chartok_coords(self, idx, ref, smiles, coords=None, edges=None, mask_ratio=0):\n",
    "        max_len = FORMAT_INFO['chartok_coords']['max_len']\n",
    "        tokenizer = self.tokenizer['chartok_coords']\n",
    "        if smiles is None or type(smiles) is not str:\n",
    "            smiles = \"\"\n",
    "        label, indices = tokenizer.smiles_to_sequence(smiles, coords, mask_ratio=mask_ratio)\n",
    "        ref['chartok_coords'] = torch.LongTensor(label[:max_len])\n",
    "        indices = [i for i in indices if i < max_len]\n",
    "        ref['atom_indices'] = torch.LongTensor(indices)\n",
    "        if tokenizer.continuous_coords:\n",
    "            if coords is not None:\n",
    "                ref['coords'] = torch.tensor(coords)\n",
    "            else:\n",
    "                ref['coords'] = torch.ones(len(indices), 2) * -1.\n",
    "        if edges is not None:\n",
    "            ref['edges'] = torch.tensor(edges)[:len(indices), :len(indices)]\n",
    "        else:\n",
    "            if 'edges' in self.df.columns:\n",
    "                edge_list = eval(self.df.loc[idx, 'edges'])\n",
    "                n = len(indices)\n",
    "                edges = torch.zeros((n, n), dtype=torch.long)\n",
    "                for u, v, t in edge_list:\n",
    "                    if u < n and v < n:\n",
    "                        if t <= 4:\n",
    "                            edges[u, v] = t\n",
    "                            edges[v, u] = t\n",
    "                        else:\n",
    "                            edges[u, v] = t\n",
    "                            edges[v, u] = 11 - t\n",
    "                ref['edges'] = edges\n",
    "            else:\n",
    "                ref['edges'] = torch.ones(len(indices), len(indices), dtype=torch.long) * (-100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdc6cc-fd7f-40e6-a27c-9604d95dfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NEW: map element indices (from AFMData) to atomic symbols used by tokenizer\n",
    "INDEX_TO_SYMBOL = {\n",
    "    0: 'H',\n",
    "    1: 'C',\n",
    "    2: 'N',\n",
    "    3: 'O',\n",
    "    4: 'F',\n",
    "}\n",
    "\n",
    "class AFM3DToChartokConverter:\n",
    "    \"\"\"3D AFM to ChartTok converter for training\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, coord_bins=64, default_atom='C', use_3d=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.coord_bins = coord_bins\n",
    "        self.default_atom = default_atom\n",
    "        self.use_3d = use_3d\n",
    "        \n",
    "        # Create coordinate tokens (bins for X, Y, Z)\n",
    "        self.coord_tokens = [f'<COORD_{i}>' for i in range(coord_bins)]\n",
    "    \n",
    "    def coords_3d_to_token_sequence(self, coords_3d, symbols):\n",
    "        \"\"\"Convert 3D coordinates and symbols to token sequence (symbols + SOS/EOS).\"\"\"\n",
    "        if coords_3d is None or len(coords_3d) == 0 or len(symbols) == 0:\n",
    "            return torch.tensor([1, 2], dtype=torch.long)  # SOS, EOS\n",
    "        \n",
    "        sequence = [1]  # SOS token\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            if symbol in self.tokenizer.stoi:\n",
    "                sequence.append(self.tokenizer.stoi[symbol])\n",
    "            else:\n",
    "                sequence.append(self.tokenizer.stoi.get(self.default_atom, UNK_ID))\n",
    "        \n",
    "        sequence.append(2)  # EOS\n",
    "        return torch.tensor(sequence, dtype=torch.long)\n",
    "    \n",
    "    def _normalize_coords_3d(self, coords):\n",
    "        \"\"\"Normalize 3D coordinates to [0, 1] range\"\"\"\n",
    "        coords = np.array(coords)\n",
    "        if len(coords) == 0:\n",
    "            return coords\n",
    "        \n",
    "        # Per-dimension normalization\n",
    "        coords_norm = np.zeros_like(coords)\n",
    "        for dim in range(3):\n",
    "            coord_dim = coords[:, dim]\n",
    "            min_val, max_val = coord_dim.min(), coord_dim.max()\n",
    "            if max_val > min_val:\n",
    "                coords_norm[:, dim] = (coord_dim - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                coords_norm[:, dim] = 0.5  # Default to middle if all same\n",
    "        \n",
    "        return coords_norm\n",
    "    \n",
    "    def convert_single_molecule_3d(self, nodes, edges=None):\n",
    "        \"\"\"Convert single AFM molecule to 3D chartok format\"\"\"\n",
    "        if hasattr(nodes, 'x') and hasattr(nodes, 'pos'):\n",
    "            # PyTorch Geometric format\n",
    "            symbols = [self.default_atom] * len(nodes.x)  # Simplified - use default atom\n",
    "            coords_3d = nodes.pos.numpy() if hasattr(nodes.pos, 'numpy') else nodes.pos\n",
    "        elif isinstance(nodes, dict):\n",
    "            # Dictionary format\n",
    "            if 'pos' in nodes and 'x' in nodes:\n",
    "                coords_3d = nodes['pos']\n",
    "                symbols = [self.default_atom] * len(coords_3d)\n",
    "            else:\n",
    "                # Default fallback\n",
    "                coords_3d = np.random.rand(5, 3)  # Random 3D positions\n",
    "                symbols = [self.default_atom] * 5\n",
    "        else:\n",
    "            # Tensor or array format\n",
    "            if hasattr(nodes, 'shape') and len(nodes.shape) >= 2:\n",
    "                if nodes.shape[-1] >= 3:\n",
    "                    coords_3d = nodes[:, :3] if len(nodes.shape) == 2 else nodes.reshape(-1, 3)[:, :3]\n",
    "                    symbols = [self.default_atom] * len(coords_3d)\n",
    "                else:\n",
    "                    # Fallback to random\n",
    "                    coords_3d = np.random.rand(5, 3)\n",
    "                    symbols = [self.default_atom] * 5\n",
    "            else:\n",
    "                coords_3d = np.random.rand(5, 3)\n",
    "                symbols = [self.default_atom] * 5\n",
    "        \n",
    "        # Convert to token sequence\n",
    "        token_sequence = self.coords_3d_to_token_sequence(coords_3d, symbols)\n",
    "        \n",
    "        # Create info dict\n",
    "        nodes_3d_dict = {\n",
    "            'coords_3d': coords_3d.tolist() if hasattr(coords_3d, 'tolist') else coords_3d,\n",
    "            'symbols': symbols,\n",
    "            'num_atoms': len(symbols)\n",
    "        }\n",
    "        \n",
    "        return token_sequence, nodes_3d_dict\n",
    "\n",
    "# NEW: Dataset wrapper that uses AFMData (HDF5) and converts to model-friendly items\n",
    "class H5AFM3DDataset(Dataset):\n",
    "    def __init__(self, h5_path, tokenizer, coord_bins=64):\n",
    "        super().__init__()\n",
    "        self.afm = AFMData(h5_path, transform=None, train_size=1.0, split='train')\n",
    "        self.converter = AFM3DToChartokConverter(tokenizer, coord_bins=coord_bins, default_atom='C', use_3d=True)\n",
    "    def __len__(self):\n",
    "        return len(self.afm)\n",
    "    def __getitem__(self, idx):\n",
    "        idx_out, image, sample = self.afm[idx]\n",
    "        # sample['coords']: numpy array [num_atoms, 5] -> x,y,z,charge?,element_index(0..4)\n",
    "        coords_np = sample['coords']\n",
    "        edges_np = sample['edges']  # (E, 2)\n",
    "        coords_3d = torch.from_numpy(coords_np[:, :3]).float() if coords_np.size > 0 else torch.zeros((0, 3), dtype=torch.float32)\n",
    "        # derive symbols from last column (already mapped to 0..4)\n",
    "        elem_idx = coords_np[:, -1].astype(int) if coords_np.size > 0 else np.array([], dtype=int)\n",
    "        symbols = [INDEX_TO_SYMBOL.get(int(i), 'C') for i in elem_idx]\n",
    "        # build token sequence for chartok_coords\n",
    "        sequence = self.converter.coords_3d_to_token_sequence(coords_3d.numpy(), symbols)\n",
    "        return {\n",
    "            'idx': torch.tensor(idx_out, dtype=torch.long),\n",
    "            'image': image,                         # tensor [3,H,W]\n",
    "            'sequence': sequence,                   # tensor [T]\n",
    "            'seq_length': torch.tensor(len(sequence), dtype=torch.long),\n",
    "            'coords_3d': coords_3d,                 # tensor [N,3]\n",
    "            'symbols': symbols,\n",
    "            'num_atoms': torch.tensor(len(symbols), dtype=torch.long),\n",
    "            'edges_list': torch.from_numpy(edges_np).long() if edges_np.size > 0 else torch.zeros((0, 2), dtype=torch.long),\n",
    "        }\n",
    "\n",
    "class Complete3DAFMDataset:\n",
    "    \"\"\"Complete 3D AFM dataset wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, afm_dataset, tokenizer, coord_bins=64, default_atom='C', use_3d=True):\n",
    "        self.afm_dataset = afm_dataset\n",
    "        self.converter = AFM3DToChartokConverter(tokenizer, coord_bins, default_atom, use_3d)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.afm_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        # Get AFM sample\n",
    "        afm_sample = self.afm_dataset[idx]\n",
    "        \n",
    "        # Extract molecular data\n",
    "        nodes = afm_sample.get('nodes', afm_sample.get('x', None))\n",
    "        edges = afm_sample.get('edges', None)\n",
    "        \n",
    "        if nodes is not None:\n",
    "            chartok_3d_seq, nodes_3d_dict = self.converter.convert_single_molecule_3d(nodes, edges)\n",
    "            \n",
    "            converted_sample = {\n",
    "                'idx': idx,\n",
    "                'chartok_coords_3d': chartok_3d_seq,\n",
    "                'nodes_3d_dict': nodes_3d_dict,\n",
    "                'original_afm_sample': afm_sample,\n",
    "                'coords_3d': torch.tensor(nodes_3d_dict['coords_3d'], dtype=torch.float32),\n",
    "                'symbols': nodes_3d_dict['symbols']\n",
    "            }\n",
    "        else:\n",
    "            # Fallback\n",
    "            converted_sample = {\n",
    "                'idx': idx,\n",
    "                'chartok_coords_3d': torch.tensor([1, 2]),\n",
    "                'nodes_3d_dict': {'coords_3d': [], 'symbols': []},\n",
    "                'original_afm_sample': afm_sample,\n",
    "                'coords_3d': torch.tensor([]),\n",
    "                'symbols': []\n",
    "            }\n",
    "        \n",
    "        self.cache[idx] = converted_sample\n",
    "        return converted_sample\n",
    "\n",
    "class AFM3DCollator:\n",
    "    \"\"\"Custom collate function for 3D AFM data\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        \n",
    "        sequences = [sample['chartok_coords_3d'] for sample in batch]\n",
    "        coords_3d_list = [sample['coords_3d'] for sample in batch]\n",
    "        symbols_list = [sample['symbols'] for sample in batch]\n",
    "        \n",
    "        # Pad sequences\n",
    "        max_seq_len = min(max(len(seq) for seq in sequences), self.max_length)\n",
    "        padded_sequences = torch.zeros((batch_size, max_seq_len), dtype=torch.long)\n",
    "        attention_masks = torch.zeros((batch_size, max_seq_len), dtype=torch.bool)\n",
    "        \n",
    "        # 3D coordinates\n",
    "        max_atoms = max(len(coords) for coords in coords_3d_list) if coords_3d_list[0].numel() > 0 else 1\n",
    "        coords_3d_batch = torch.zeros((batch_size, max_atoms, 3))\n",
    "        coords_mask = torch.zeros((batch_size, max_atoms), dtype=torch.bool)\n",
    "        \n",
    "        for i, (seq, coords_3d, symbols) in enumerate(zip(sequences, coords_3d_list, symbols_list)):\n",
    "            seq_len = min(len(seq), max_seq_len)\n",
    "            padded_sequences[i, :seq_len] = seq[:seq_len]\n",
    "            attention_masks[i, :seq_len] = True\n",
    "            \n",
    "            if coords_3d.numel() > 0 and len(coords_3d.shape) == 2:\n",
    "                atom_len = min(len(coords_3d), max_atoms)\n",
    "                coords_3d_batch[i, :atom_len] = coords_3d[:atom_len]\n",
    "                coords_mask[i, :atom_len] = True\n",
    "        \n",
    "        return {\n",
    "            'input_ids': padded_sequences,\n",
    "            'attention_mask': attention_masks,\n",
    "            'coords_3d': coords_3d_batch,\n",
    "            'coords_mask': coords_mask,\n",
    "            'symbols_batch': symbols_list,\n",
    "            'batch_size': batch_size,\n",
    "            'max_seq_len': max_seq_len,\n",
    "            'max_atoms': max_atoms\n",
    "        }\n",
    "\n",
    "def make_collate_fn(tokenizer):\n",
    "    \"\"\"Create a collate function that maps out-of-range tokens to UNK and batches coords/edges.\"\"\"\n",
    "    vocab_size = len(tokenizer)\n",
    "    base_vocab = len(tokenizer.stoi)\n",
    "\n",
    "    def _collate(batch):\n",
    "        # indices and images\n",
    "        indices = torch.stack([item['idx'] for item in batch])\n",
    "        images = torch.stack([item['image'] for item in batch])\n",
    "\n",
    "        # sequences with UNK clamping\n",
    "        raw_sequences = [item['sequence'] for item in batch]\n",
    "        sequences = []\n",
    "        seq_lengths = []\n",
    "        for seq in raw_sequences:\n",
    "            seq = seq.clone()\n",
    "            invalid = (seq >= vocab_size) | (seq < 0)\n",
    "            if invalid.any():\n",
    "                seq[invalid] = UNK_ID\n",
    "            sequences.append(seq)\n",
    "            seq_lengths.append(torch.tensor(len(seq), dtype=torch.long))\n",
    "        seq_lengths = torch.stack(seq_lengths)\n",
    "\n",
    "        # debug token range for first few batches\n",
    "        all_tokens = torch.cat(sequences) if len(sequences) > 0 else torch.tensor([], dtype=torch.long)\n",
    "        if all_tokens.numel() > 0:\n",
    "            max_token = all_tokens.max().item(); min_token = all_tokens.min().item()\n",
    "        else:\n",
    "            max_token = min_token = 0\n",
    "        if not hasattr(_collate, 'call_count'):\n",
    "            _collate.call_count = 0\n",
    "        _collate.call_count += 1\n",
    "        if _collate.call_count <= 3:\n",
    "            logger.info(f\"ðŸ” Batch {_collate.call_count}: Token range [{min_token}, {max_token}], base_vocab={base_vocab}, total_vocab={vocab_size}, sequences={len(sequences)}\")\n",
    "\n",
    "        # pad sequences\n",
    "        max_len = max(len(seq) for seq in sequences) if sequences else 0\n",
    "        batch_size = len(batch)\n",
    "        padded_sequences = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            L = len(seq)\n",
    "            if L > 0:\n",
    "                padded_sequences[i, :L] = seq\n",
    "\n",
    "        # coords_3d batching (pad to max atoms), use -100 for masked positions\n",
    "        coords_list = [item['coords_3d'] for item in batch]\n",
    "        num_atoms_list = [int(item['num_atoms']) for item in batch]\n",
    "        max_atoms = max(num_atoms_list) if num_atoms_list else 0\n",
    "        coords_3d_batch = torch.full((batch_size, max_atoms, 3), -100.0, dtype=torch.float32)\n",
    "        for i, coords in enumerate(coords_list):\n",
    "            n = min(coords.size(0), max_atoms)\n",
    "            if n > 0:\n",
    "                coords_3d_batch[i, :n, :] = coords[:n, :]\n",
    "\n",
    "        # atom_indices: positions of symbol tokens in sequence: [1..num_atoms]\n",
    "        atom_indices = torch.zeros(batch_size, max_atoms, dtype=torch.long)\n",
    "        for i, n_atoms in enumerate(num_atoms_list):\n",
    "            n = min(n_atoms, max_atoms)\n",
    "            if n > 0:\n",
    "                atom_indices[i, :n] = torch.arange(1, 1 + n)\n",
    "\n",
    "        # edges target placeholder: ignore all pairs by default (-100), shape [B, max_atoms, max_atoms]\n",
    "        edges_target = torch.full((batch_size, max_atoms, max_atoms), -100, dtype=torch.long)\n",
    "\n",
    "        refs = {\n",
    "            'chartok_coords': (padded_sequences, seq_lengths),\n",
    "            'coords_3d': coords_3d_batch,\n",
    "            'edges': edges_target,\n",
    "            'atom_indices': (atom_indices,),\n",
    "        }\n",
    "        return indices, images, refs\n",
    "\n",
    "    return _collate\n",
    "\n",
    "def create_3d_dataloader(data_path, batch_size=16, num_workers=4):\n",
    "    \"\"\"Create DataLoader for 3D AFM training from HDF5 using the provided pipeline\"\"\"\n",
    "    args = create_model_args()\n",
    "\n",
    "    # Determine HDF5 file path\n",
    "    h5_path = data_path\n",
    "    if os.path.isdir(data_path):\n",
    "        # pick the first .h5 file in the directory\n",
    "        h5_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.h5') or f.endswith('.hdf5')]\n",
    "        if len(h5_files) == 0:\n",
    "            raise FileNotFoundError(f\"No .h5/.hdf5 files found under {data_path}\")\n",
    "        h5_path = sorted(h5_files)[0]\n",
    "\n",
    "    # Build tokenizer(s)\n",
    "    tokenizers = get_tokenizer(args)\n",
    "    chartok_tok = tokenizers['chartok_coords']\n",
    "    logger.info(f\"ðŸ“š Tokenizer built: chartok_coords | base_vocab={len(chartok_tok.stoi)} | total_vocab={len(chartok_tok)} | coord_bins={args.coord_bins}\")\n",
    "\n",
    "    # Create dataset from HDF5\n",
    "    dataset = H5AFM3DDataset(h5_path, chartok_tok, coord_bins=args.coord_bins)\n",
    "    logger.info(f\"ðŸ“‚ Using AFM HDF5 dataset: {h5_path} | {len(dataset)} samples\")\n",
    "\n",
    "    # Collate function\n",
    "    collate_fn = make_collate_fn(chartok_tok)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    logger.info(f\"âœ… DataLoader created with {len(dataset)} samples, {len(train_loader)} batches\")\n",
    "    return train_loader, tokenizers, dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d7d95-c2d0-457d-a4fb-0dd794755ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolNexTR_env",
   "language": "python",
   "name": "molnextr_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
