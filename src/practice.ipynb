{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62315e7d-858d-42e2-a890-ad27141274e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source /scratch/phys/sin/sethih1/venv/MolNexTR_env/bin/activate\n",
    "import sys\n",
    "sys.path.append('/home/sethih1/MORAFInator/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c625705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import time \n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.optim import Adam, AdamW, SGD\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from src.components import Encoder, Decoder\n",
    "from src.loss_fuc import Criterion\n",
    "from src.utils import seed_torch, save_args, init_summary_writer, LossMeter, AverageMeter, asMinutes, timeSince, print_rank_0, format_df\n",
    "from src.chemical import convert_graph_to_smiles, postprocess_smiles, keep_main_molecule\n",
    "from src.tokenization import get_tokenizer\n",
    "#from src.dataloader.data_loader import get_datasets, afm_collate_fn\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "403f5a58-22f3-4f3a-953e-b48a3d54bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.02 0.03 0.05 0.06 0.08 0.1  0.11 0.13 0.14 0.16 0.17 0.19 0.21\n",
      " 0.22 0.24 0.25 0.27 0.29 0.3  0.32 0.33 0.35 0.37 0.38 0.4  0.41 0.43\n",
      " 0.44 0.46 0.48 0.49 0.51 0.52 0.54 0.56 0.57 0.59 0.6  0.62 0.63 0.65\n",
      " 0.67 0.68 0.7  0.71 0.73 0.75 0.76 0.78 0.79 0.81 0.83 0.84 0.86 0.87\n",
      " 0.89 0.9  0.92 0.94 0.95 0.97 0.98 1.  ]\n"
     ]
    }
   ],
   "source": [
    "coord_bins = 64\n",
    "\n",
    "coord_bin_values = np.round(np.linspace(0,1,64),2)\n",
    "print(coord_bin_values)\n",
    "coord_tokens = [f'<COORD_{i}>' for i in range(coord_bins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3337aef1-0793-4a17-8b29-4961487d919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COVALENT_RADII = {\n",
    "    1: 0.32,\n",
    "    6: 0.77,\n",
    "    7: 0.75,\n",
    "    8: 0.73,\n",
    "    9: 0.71,\n",
    "}\n",
    "\n",
    "ELEMENT_TO_INDEX = {\n",
    "    1: 0,\n",
    "    6: 1,\n",
    "    7: 2,\n",
    "    8: 3,\n",
    "    9: 4,\n",
    "}\n",
    "\n",
    "INDEX_TO_SYMBOL = {\n",
    "    1: 'H',\n",
    "    6: 'C',\n",
    "    7: 'N',\n",
    "    8: 'O',\n",
    "    9: 'F',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "INDEX_TO_SYMBOL = {\n",
    "    0: 'H',\n",
    "    1: 'C',\n",
    "    2: 'N',\n",
    "    3: 'O',\n",
    "    4: 'F',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d74d6d53-f917-487e-a465-7f0febf90b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "\n",
    "class AFMDataset(Dataset): \n",
    "    def __init__(self, data_path, tokenizer, transform, train_size=0.8, split='train'): \n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with h5py.File(self.data_path, 'r') as f:\n",
    "            total_length = f['x'].shape[0]\n",
    "            self.train_length = int(train_size*total_length)\n",
    "            self.val_length = total_length - self.train_length\n",
    "\n",
    "    def __len__(self): \n",
    "        if self.split == 'train': \n",
    "            return self.train_length\n",
    "        else: \n",
    "            return self.val_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.split == 'train': \n",
    "            idx += 0\n",
    "        else: \n",
    "            idx += self.train_length\n",
    "\n",
    "        with h5py.File(self.data_path, 'r') as f:\n",
    "            x = f['x'][idx]\n",
    "            xyz = f['xyz'][idx]\n",
    "\n",
    "        # Remove padding atoms\n",
    "        xyz = xyz[xyz[:, -1] > 0]\n",
    "\n",
    "        # Get edges \n",
    "        edges = []\n",
    "        for i in range(xyz.shape[0]): \n",
    "            for j in range(i+1, xyz.shape[0]): \n",
    "                dist = np.linalg.norm(xyz[i, :3] - xyz[j, :3])\n",
    "                if dist < 1.2*(COVALENT_RADII[xyz[i, -1]] + COVALENT_RADII[xyz[j, -1]]):\n",
    "                    edges.append([i,j])\n",
    "\n",
    "        # Normalize xyz to [0.25, 0.75]\n",
    "        xyzmin = np.min(xyz[:, :3])\n",
    "        xyz_max = np.max(xyz[:, 3])\n",
    "\n",
    "        xyz[:, :3] = (xyz[:, :3] - xyzmin)/(xyz_max - xyzmin)\n",
    "        xyz[:, :3] = 0.5*xyz[:, :3] + 0.25\n",
    "\n",
    "        # map atom types to integers (0,1, ..)\n",
    "        xyz[:, -1] = [ELEMENT_TO_INDEX[atom_type] for atom_type in xyz[:, -1]]\n",
    "\n",
    "        #sample = {'coords': xyz, edges: np.asarray(edges)}\n",
    "\n",
    "        #if self.transform:\n",
    "        #    sample = self.transform(sample)\n",
    "\n",
    "        # Keep all channels from HDF5 and convert to [C,H,W]\n",
    "        x = torch.from_numpy(x)\n",
    "        if x.dim() == 4 and x.size(0) == 1: \n",
    "            # Input like [1, H, W, C] -> [H, W, C]\n",
    "            x = x.squeeze(0)\n",
    "        if x.dim() == 3: \n",
    "            # Assume [H, W, C] from HDF5, move channels first\n",
    "            x = x.permute(2,0,1).contiguous()\n",
    "\n",
    "        mask = xyz[:, -1] > 0\n",
    "        atomtok = \" \".join([INDEX_TO_SYMBOL[value] for value in xyz[mask, -1]])\n",
    "\n",
    "        #atomtok = self.tokenizer['atomtok'].text_to_sequence(atomtok)\n",
    "\n",
    "        nodes = {'coords': [], 'symbols': \"\"}\n",
    "        for symbol, coord in zip(atomtok, xyz):\n",
    "            new_coord = coord_bin_values[np.argmin(np.abs(coord_bin_values.reshape(1,-1) - coord.reshape(-1,1)), axis = 1)]\n",
    "            #atomtok_coords.append(f'{symbol}: {new_coord[0]}, {new_coord[1]}, {new_coord[2]},')\n",
    "            nodes['coords'].append(new_coord[:2])\n",
    "            nodes['symbols'] = symbol + \" \"\n",
    "\n",
    "        atomtok_coords = self.tokenizer['atomtok_coords'].nodes_to_sequence(nodes)\n",
    "\n",
    "        #ref = {'atomtok': atomtok, 'edges': np.asarray(edges), 'atomtok_coords': atomtok_coords, 'chartok_coords': atomtok_coords}\n",
    "        #ref = {'atomtok_coords': atomtok_coords, 'atom_indices': , 'coords': nodes['coords'], 'edges': edges}\n",
    "        ref = {'atomtok_coords': atomtok_coords}\n",
    "        return idx, x, ref\n",
    "\n",
    "\n",
    "def get_datasets(data_path, tokenizer, train_transform = None, val_transform = None, train_size=0.8):\n",
    "\n",
    "    train_dataset = AFMDataset(data_path, tokenizer = tokenizer, transform=train_transform, train_size=train_size, split='train')\n",
    "    val_dataset = AFMDataset(data_path, tokenizer = tokenizer, transform=val_transform, train_size=train_size, split='val')\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def afm_collate_fn(batch):\n",
    "\n",
    "    #sample = {'coords':[], 'edges':[]}\n",
    "    ref = {'atomtok': [], 'edges': [], 'atomtok_coords': [], 'chartok_coords': []}\n",
    "    ref = {'atomtok': []}\n",
    "    #ref = {'atomtok_coords': []}\n",
    "\n",
    "    PAD_ID = 0\n",
    "    length = 128\n",
    "\n",
    "    ids = [id[0] for id in batch]\n",
    "    images = torch.stack([item[1] for item in batch])\n",
    "    for item in batch:\n",
    "        #sample['coords'].append(torch.from_numpy(item[2]['coords']))\n",
    "        #sample['edges'].append(torch.from_numpy(item[2]['edges']))\n",
    "        #ref['atomtok'].append(torch.from_numpy(item[2]['atomtok']))\n",
    "        #ref['edges'].append(torch.from_numpy(item[2]['edges']))\n",
    "\n",
    "        tok = item[2]['atomtok_coords']\n",
    "        \n",
    "        pad = [PAD_ID]*(length - len(tok))\n",
    "        tok.extend(pad)\n",
    "        \n",
    "        ref['atomtok_coords'] = tok\n",
    "        #ref['atomtok_coords'].append(torch.from_numpy(item[2]['atomtok_coords']))\n",
    "        #ref['chartok_coords'].append(torch.from_numpy(item[2]['chartok_coords']))\n",
    "\n",
    "    return ids, images, ref\n",
    "    #return ids, images, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c11b2e-48a7-4f7b-9f6c-e6740d4b0553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(batch_size=32, learning_rate=0.001, encoder='swin_base', use_checkpoint=False, encoder_dim=64, in_chans=10, dec_hidden_size=16, enc_pos_emb=True, dec_num_layers=3, dec_attn_heads=4, hidden_dropout=0.2, attn_dropout=0.2, max_relative_positions=10, compute_confidence=True, formats=['atomtok_coords'], vocab_file='/home/sethih1/MORAFInator/src/vocab/vocab_chars.json', coord_bins=64, sep_xy=False, continuous_coords=True, input_resolution=256)\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args_dict = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    # related to model encoder\n",
    "    'encoder': 'swin_base', \n",
    "    'use_checkpoint':False, \n",
    "    'encoder_dim': 64, \n",
    "    'in_chans': 10,\n",
    "\n",
    "    # related to model decoder\n",
    "    'dec_hidden_size': 16,\n",
    "    'enc_pos_emb': True, \n",
    "    'dec_num_layers': 3, \n",
    "    'dec_attn_heads': 4,\n",
    "    'hidden_dropout': 0.2,\n",
    "    'attn_dropout': 0.2,\n",
    "    'max_relative_positions': 10,\n",
    "    'compute_confidence': True,\n",
    "\n",
    "    # related to tokenizer\n",
    "    'formats':['atomtok_coords'],\n",
    "    'vocab_file': '/home/sethih1/MORAFInator/src/vocab/vocab_chars.json', \n",
    "    'coord_bins': 64, \n",
    "    'sep_xy': False, \n",
    "    'continuous_coords': True, \n",
    "    'input_resolution': 256\n",
    "    \n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**args_dict)\n",
    "print(args)\n",
    "\n",
    "tokenizer = get_tokenizer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df2a4c82-b584-48ce-9540-c5c0bea29352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a5fa25-a49d-4554-83ae-80bcce21ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(args, pretrained=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d9f24fb-9702-451e-811b-bdb9915ad239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "#summary(encoder, input_size=(1, 10, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c084aa18-f4ee-41ae-892d-fff2c3fc67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim = encoder.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a13f1c-49f6-4894-9dee-68656eb87b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (decoder): ModuleDict(\n",
      "    (atomtok_coords): TransformerDecoderAR(\n",
      "      (enc_trans_layer): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=16, bias=True)\n",
      "      )\n",
      "      (enc_pos_emb): Embedding(144, 64)\n",
      "      (decoder): TransformerDecoder(\n",
      "        (layer_norm): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "        (transformer_layers): ModuleList(\n",
      "          (0-2): 3 x TransformerDecoderLayer(\n",
      "            (self_attn): MultiHeadedAttention(\n",
      "              (linear_keys): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_values): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_query): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "              (final_linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (relative_positions_embeddings): Embedding(21, 4)\n",
      "            )\n",
      "            (feed_forward): PositionwiseFeedForward(\n",
      "              (w_1): Linear(in_features=16, out_features=64, bias=True)\n",
      "              (w_2): Linear(in_features=64, out_features=16, bias=True)\n",
      "              (layer_norm): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "              (dropout_1): Dropout(p=0.2, inplace=False)\n",
      "              (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "            (layer_norm_1): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "            (context_attn): MultiHeadedAttention(\n",
      "              (linear_keys): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_values): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (linear_query): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "              (final_linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "            (layer_norm_2): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (output_layer): Linear(in_features=16, out_features=165, bias=True)\n",
      "      (embeddings): Embeddings(\n",
      "        (make_embedding): Sequential(\n",
      "          (emb_luts): Elementwise(\n",
      "            (0): Embedding(165, 16, padding_idx=0)\n",
      "          )\n",
      "          (pe): PositionalEncoding(\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(args, tokenizer)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d60f5b31-e579-488a-a477-4822827091fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = AdamW(encoder.parameters(), lr = 0.001, weight_decay=0.0001)\n",
    "decoder_optimizer = AdamW(decoder.parameters(), lr= 0.001, weight_decay= 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f75af650-0829-49f2-b722-70cac453980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path =  \"/scratch/phys/project/sin/hackathon/data/afm.h5\"\n",
    "train_dataset, val_dataset = get_datasets(data_path = h5_path, tokenizer=tokenizer, train_transform = None, val_transform = None, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae05c7b8-3f2f-40e9-92a1-2afdc95d181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=1, drop_last = False, collate_fn=afm_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95d39da7-f328-4f4d-a402-32d8d852d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, imgs, samples = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5abdd9ee-8b80-463c-9396-e3e1cd316c73",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'coords'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'coords'"
     ]
    }
   ],
   "source": [
    "samples['coords']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21a309ce-5703-4426-9b5b-03f237ac2ed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m features, hiddens \u001b[38;5;241m=\u001b[39m encoder(imgs)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#features, hiddens = encoder(imgs, refs)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhiddens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m losses \u001b[38;5;241m=\u001b[39m criterion(results, refs)\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(losses\u001b[38;5;241m.\u001b[39mvalue())\n",
      "File \u001b[0;32m/scratch/phys/sin/sethih1/venv/MolNexTR_env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/phys/sin/sethih1/venv/MolNexTR_env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/MORAFInator/src/components.py:442\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, encoder_out, hiddens, refs)\u001b[0m\n\u001b[1;32m    440\u001b[0m         results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (predictions, targets)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m         labels, label_lengths \u001b[38;5;241m=\u001b[39m refs[format_]\n\u001b[1;32m    443\u001b[0m         results[format_] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder[format_](encoder_out, labels, label_lengths)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for step, (indices, imgs, refs) in enumerate(train_loader):\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        #refs = refs.to(device)\n",
    "\n",
    "\n",
    "        features, hiddens = encoder(imgs)\n",
    "        \n",
    "        #features, hiddens = encoder(imgs, refs)\n",
    "        results = decoder(features, hiddens, refs)\n",
    "        losses = criterion(results, refs)\n",
    "        loss = sum(losses.value())\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        losses.append(loss.value())\n",
    "\n",
    "    train_loss = torch.mean(torch.tensor(losses))\n",
    "\n",
    "    print(f\"Epoch {epoch} training loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93914dfd-b10d-457a-b8a6-c48ccbf5c76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8350b0b-75ba-48a6-a2e2-12e279b79b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13073b14-4cd3-4b7c-8527-28bbc66ac820",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_3d_dataloader(h5_path, batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345fcd5-daaa-4522-9b0c-9bd765a4fb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7227fa-45af-4f68-b6f3-7ea976ee04f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    def getitem(self, idx):\n",
    "        ref = {}\n",
    "        if self.dynamic_indigo:\n",
    "            begin = time.time()\n",
    "            image, smiles, graph, success = generate_indigo_image(\n",
    "                self.smiles[idx], mol_augment=self.args.mol_augment, default_option=self.args.default_option,\n",
    "                shuffle_nodes=self.args.shuffle_nodes, pseudo_coords=self.pseudo_coords,\n",
    "                include_condensed=self.args.include_condensed)\n",
    "            # raw_image = image\n",
    "            end = time.time()\n",
    "            if idx < 30 and self.args.save_image:\n",
    "                path = os.path.join(self.args.save_path, 'images')\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                cv2.imwrite(os.path.join(path, f'{idx}.png'), image)\n",
    "            if not success:\n",
    "                return idx, None, {}\n",
    "            image, coords = self.image_transform(image, graph['coords'], renormalize=self.pseudo_coords)\n",
    "            graph['coords'] = coords\n",
    "            ref['time'] = end - begin\n",
    "            if 'atomtok' in self.formats:\n",
    "                max_len = FORMAT_INFO['atomtok']['max_len']\n",
    "                label = self.tokenizer['atomtok'].text_to_sequence(smiles, tokenized=False)\n",
    "                ref['atomtok'] = torch.LongTensor(label[:max_len])\n",
    "            if 'edges' in self.formats and 'atomtok_coords' not in self.formats and 'chartok_coords' not in self.formats:\n",
    "                ref['edges'] = torch.tensor(graph['edges'])\n",
    "            if 'atomtok_coords' in self.formats:\n",
    "                self._process_atomtok_coords(idx, ref, smiles, graph['coords'], graph['edges'],\n",
    "                                             mask_ratio=self.args.mask_ratio)\n",
    "            if 'chartok_coords' in self.formats:\n",
    "                self._process_chartok_coords(idx, ref, smiles, graph['coords'], graph['edges'],\n",
    "                                             mask_ratio=self.args.mask_ratio)\n",
    "            return idx, image, ref\n",
    "        else:\n",
    "            file_path = self.file_paths[idx]\n",
    "            image = cv2.imread(file_path)\n",
    "            if image is None:\n",
    "                image = np.array([[[255., 255., 255.]] * 10] * 10).astype(np.float32)\n",
    "                print(file_path, 'not found!')\n",
    "            if self.coords_df is not None:\n",
    "                h, w, _ = image.shape\n",
    "                coords = np.array(eval(self.coords_df.loc[idx, 'node_coords']))\n",
    "                if self.pseudo_coords:\n",
    "                    coords = normalize_nodes(coords)\n",
    "                coords[:, 0] = coords[:, 0] * w\n",
    "                coords[:, 1] = coords[:, 1] * h\n",
    "                image, coords = self.image_transform(image, coords, renormalize=self.pseudo_coords)\n",
    "            else:\n",
    "                image = self.image_transform(image)\n",
    "                coords = None\n",
    "            if self.labelled:\n",
    "                smiles = self.smiles[idx]\n",
    "                if 'atomtok' in self.formats:\n",
    "                    max_len = FORMAT_INFO['atomtok']['max_len']\n",
    "                    label = self.tokenizer['atomtok'].text_to_sequence(smiles, False)\n",
    "                    ref['atomtok'] = torch.LongTensor(label[:max_len])\n",
    "                if 'atomtok_coords' in self.formats:\n",
    "                    if coords is not None:\n",
    "                        self._process_atomtok_coords(idx, ref, smiles, coords, mask_ratio=0)\n",
    "                    else:\n",
    "                        self._process_atomtok_coords(idx, ref, smiles, mask_ratio=1)\n",
    "                if 'chartok_coords' in self.formats:\n",
    "                    if coords is not None:\n",
    "                        self._process_chartok_coords(idx, ref, smiles, coords, mask_ratio=0)\n",
    "                    else:\n",
    "                        self._process_chartok_coords(idx, ref, smiles, mask_ratio=1)\n",
    "            if self.args.predict_coords and ('atomtok_coords' in self.formats or 'chartok_coords' in self.formats):\n",
    "                smiles = self.smiles[idx]\n",
    "                if 'atomtok_coords' in self.formats:\n",
    "                    self._process_atomtok_coords(idx, ref, smiles, mask_ratio=1)\n",
    "                if 'chartok_coords' in self.formats:\n",
    "                    self._process_chartok_coords(idx, ref, smiles, mask_ratio=1)\n",
    "            return idx, image, ref\n",
    "\n",
    "    def _process_atomtok_coords(self, idx, ref, smiles, coords=None, edges=None, mask_ratio=0):\n",
    "        max_len = FORMAT_INFO['atomtok_coords']['max_len']\n",
    "        tokenizer = self.tokenizer['atomtok_coords']\n",
    "        if smiles is None or type(smiles) is not str:\n",
    "            smiles = \"\"\n",
    "        label, indices = tokenizer.smiles_to_sequence(smiles, coords, mask_ratio=mask_ratio)\n",
    "        ref['atomtok_coords'] = torch.LongTensor(label[:max_len])\n",
    "        indices = [i for i in indices if i < max_len]\n",
    "        ref['atom_indices'] = torch.LongTensor(indices)\n",
    "        if tokenizer.continuous_coords:\n",
    "            if coords is not None:\n",
    "                ref['coords'] = torch.tensor(coords)\n",
    "            else:\n",
    "                ref['coords'] = torch.ones(len(indices), 2) * -1.\n",
    "        if edges is not None:\n",
    "            ref['edges'] = torch.tensor(edges)[:len(indices), :len(indices)]\n",
    "        else:\n",
    "            if 'edges' in self.df.columns:\n",
    "                edge_list = eval(self.df.loc[idx, 'edges'])\n",
    "                n = len(indices)\n",
    "                edges = torch.zeros((n, n), dtype=torch.long)\n",
    "                for u, v, t in edge_list:\n",
    "                    if u < n and v < n:\n",
    "                        if t <= 4:\n",
    "                            edges[u, v] = t\n",
    "                            edges[v, u] = t\n",
    "                        else:\n",
    "                            edges[u, v] = t\n",
    "                            edges[v, u] = 11 - t\n",
    "                ref['edges'] = edges\n",
    "            else:\n",
    "                ref['edges'] = torch.ones(len(indices), len(indices), dtype=torch.long) * (-100)\n",
    "\n",
    "    def _process_chartok_coords(self, idx, ref, smiles, coords=None, edges=None, mask_ratio=0):\n",
    "        max_len = FORMAT_INFO['chartok_coords']['max_len']\n",
    "        tokenizer = self.tokenizer['chartok_coords']\n",
    "        if smiles is None or type(smiles) is not str:\n",
    "            smiles = \"\"\n",
    "        label, indices = tokenizer.smiles_to_sequence(smiles, coords, mask_ratio=mask_ratio)\n",
    "        ref['chartok_coords'] = torch.LongTensor(label[:max_len])\n",
    "        indices = [i for i in indices if i < max_len]\n",
    "        ref['atom_indices'] = torch.LongTensor(indices)\n",
    "        if tokenizer.continuous_coords:\n",
    "            if coords is not None:\n",
    "                ref['coords'] = torch.tensor(coords)\n",
    "            else:\n",
    "                ref['coords'] = torch.ones(len(indices), 2) * -1.\n",
    "        if edges is not None:\n",
    "            ref['edges'] = torch.tensor(edges)[:len(indices), :len(indices)]\n",
    "        else:\n",
    "            if 'edges' in self.df.columns:\n",
    "                edge_list = eval(self.df.loc[idx, 'edges'])\n",
    "                n = len(indices)\n",
    "                edges = torch.zeros((n, n), dtype=torch.long)\n",
    "                for u, v, t in edge_list:\n",
    "                    if u < n and v < n:\n",
    "                        if t <= 4:\n",
    "                            edges[u, v] = t\n",
    "                            edges[v, u] = t\n",
    "                        else:\n",
    "                            edges[u, v] = t\n",
    "                            edges[v, u] = 11 - t\n",
    "                ref['edges'] = edges\n",
    "            else:\n",
    "                ref['edges'] = torch.ones(len(indices), len(indices), dtype=torch.long) * (-100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdc6cc-fd7f-40e6-a27c-9604d95dfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NEW: map element indices (from AFMData) to atomic symbols used by tokenizer\n",
    "INDEX_TO_SYMBOL = {\n",
    "    0: 'H',\n",
    "    1: 'C',\n",
    "    2: 'N',\n",
    "    3: 'O',\n",
    "    4: 'F',\n",
    "}\n",
    "\n",
    "class AFM3DToChartokConverter:\n",
    "    \"\"\"3D AFM to ChartTok converter for training\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, coord_bins=64, default_atom='C', use_3d=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.coord_bins = coord_bins\n",
    "        self.default_atom = default_atom\n",
    "        self.use_3d = use_3d\n",
    "        \n",
    "        # Create coordinate tokens (bins for X, Y, Z)\n",
    "        self.coord_tokens = [f'<COORD_{i}>' for i in range(coord_bins)]\n",
    "    \n",
    "    def coords_3d_to_token_sequence(self, coords_3d, symbols):\n",
    "        \"\"\"Convert 3D coordinates and symbols to token sequence (symbols + SOS/EOS).\"\"\"\n",
    "        if coords_3d is None or len(coords_3d) == 0 or len(symbols) == 0:\n",
    "            return torch.tensor([1, 2], dtype=torch.long)  # SOS, EOS\n",
    "        \n",
    "        sequence = [1]  # SOS token\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            if symbol in self.tokenizer.stoi:\n",
    "                sequence.append(self.tokenizer.stoi[symbol])\n",
    "            else:\n",
    "                sequence.append(self.tokenizer.stoi.get(self.default_atom, UNK_ID))\n",
    "        \n",
    "        sequence.append(2)  # EOS\n",
    "        return torch.tensor(sequence, dtype=torch.long)\n",
    "    \n",
    "    def _normalize_coords_3d(self, coords):\n",
    "        \"\"\"Normalize 3D coordinates to [0, 1] range\"\"\"\n",
    "        coords = np.array(coords)\n",
    "        if len(coords) == 0:\n",
    "            return coords\n",
    "        \n",
    "        # Per-dimension normalization\n",
    "        coords_norm = np.zeros_like(coords)\n",
    "        for dim in range(3):\n",
    "            coord_dim = coords[:, dim]\n",
    "            min_val, max_val = coord_dim.min(), coord_dim.max()\n",
    "            if max_val > min_val:\n",
    "                coords_norm[:, dim] = (coord_dim - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                coords_norm[:, dim] = 0.5  # Default to middle if all same\n",
    "        \n",
    "        return coords_norm\n",
    "    \n",
    "    def convert_single_molecule_3d(self, nodes, edges=None):\n",
    "        \"\"\"Convert single AFM molecule to 3D chartok format\"\"\"\n",
    "        if hasattr(nodes, 'x') and hasattr(nodes, 'pos'):\n",
    "            # PyTorch Geometric format\n",
    "            symbols = [self.default_atom] * len(nodes.x)  # Simplified - use default atom\n",
    "            coords_3d = nodes.pos.numpy() if hasattr(nodes.pos, 'numpy') else nodes.pos\n",
    "        elif isinstance(nodes, dict):\n",
    "            # Dictionary format\n",
    "            if 'pos' in nodes and 'x' in nodes:\n",
    "                coords_3d = nodes['pos']\n",
    "                symbols = [self.default_atom] * len(coords_3d)\n",
    "            else:\n",
    "                # Default fallback\n",
    "                coords_3d = np.random.rand(5, 3)  # Random 3D positions\n",
    "                symbols = [self.default_atom] * 5\n",
    "        else:\n",
    "            # Tensor or array format\n",
    "            if hasattr(nodes, 'shape') and len(nodes.shape) >= 2:\n",
    "                if nodes.shape[-1] >= 3:\n",
    "                    coords_3d = nodes[:, :3] if len(nodes.shape) == 2 else nodes.reshape(-1, 3)[:, :3]\n",
    "                    symbols = [self.default_atom] * len(coords_3d)\n",
    "                else:\n",
    "                    # Fallback to random\n",
    "                    coords_3d = np.random.rand(5, 3)\n",
    "                    symbols = [self.default_atom] * 5\n",
    "            else:\n",
    "                coords_3d = np.random.rand(5, 3)\n",
    "                symbols = [self.default_atom] * 5\n",
    "        \n",
    "        # Convert to token sequence\n",
    "        token_sequence = self.coords_3d_to_token_sequence(coords_3d, symbols)\n",
    "        \n",
    "        # Create info dict\n",
    "        nodes_3d_dict = {\n",
    "            'coords_3d': coords_3d.tolist() if hasattr(coords_3d, 'tolist') else coords_3d,\n",
    "            'symbols': symbols,\n",
    "            'num_atoms': len(symbols)\n",
    "        }\n",
    "        \n",
    "        return token_sequence, nodes_3d_dict\n",
    "\n",
    "# NEW: Dataset wrapper that uses AFMData (HDF5) and converts to model-friendly items\n",
    "class H5AFM3DDataset(Dataset):\n",
    "    def __init__(self, h5_path, tokenizer, coord_bins=64):\n",
    "        super().__init__()\n",
    "        self.afm = AFMData(h5_path, transform=None, train_size=1.0, split='train')\n",
    "        self.converter = AFM3DToChartokConverter(tokenizer, coord_bins=coord_bins, default_atom='C', use_3d=True)\n",
    "    def __len__(self):\n",
    "        return len(self.afm)\n",
    "    def __getitem__(self, idx):\n",
    "        idx_out, image, sample = self.afm[idx]\n",
    "        # sample['coords']: numpy array [num_atoms, 5] -> x,y,z,charge?,element_index(0..4)\n",
    "        coords_np = sample['coords']\n",
    "        edges_np = sample['edges']  # (E, 2)\n",
    "        coords_3d = torch.from_numpy(coords_np[:, :3]).float() if coords_np.size > 0 else torch.zeros((0, 3), dtype=torch.float32)\n",
    "        # derive symbols from last column (already mapped to 0..4)\n",
    "        elem_idx = coords_np[:, -1].astype(int) if coords_np.size > 0 else np.array([], dtype=int)\n",
    "        symbols = [INDEX_TO_SYMBOL.get(int(i), 'C') for i in elem_idx]\n",
    "        # build token sequence for chartok_coords\n",
    "        sequence = self.converter.coords_3d_to_token_sequence(coords_3d.numpy(), symbols)\n",
    "        return {\n",
    "            'idx': torch.tensor(idx_out, dtype=torch.long),\n",
    "            'image': image,                         # tensor [3,H,W]\n",
    "            'sequence': sequence,                   # tensor [T]\n",
    "            'seq_length': torch.tensor(len(sequence), dtype=torch.long),\n",
    "            'coords_3d': coords_3d,                 # tensor [N,3]\n",
    "            'symbols': symbols,\n",
    "            'num_atoms': torch.tensor(len(symbols), dtype=torch.long),\n",
    "            'edges_list': torch.from_numpy(edges_np).long() if edges_np.size > 0 else torch.zeros((0, 2), dtype=torch.long),\n",
    "        }\n",
    "\n",
    "class Complete3DAFMDataset:\n",
    "    \"\"\"Complete 3D AFM dataset wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, afm_dataset, tokenizer, coord_bins=64, default_atom='C', use_3d=True):\n",
    "        self.afm_dataset = afm_dataset\n",
    "        self.converter = AFM3DToChartokConverter(tokenizer, coord_bins, default_atom, use_3d)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.afm_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        # Get AFM sample\n",
    "        afm_sample = self.afm_dataset[idx]\n",
    "        \n",
    "        # Extract molecular data\n",
    "        nodes = afm_sample.get('nodes', afm_sample.get('x', None))\n",
    "        edges = afm_sample.get('edges', None)\n",
    "        \n",
    "        if nodes is not None:\n",
    "            chartok_3d_seq, nodes_3d_dict = self.converter.convert_single_molecule_3d(nodes, edges)\n",
    "            \n",
    "            converted_sample = {\n",
    "                'idx': idx,\n",
    "                'chartok_coords_3d': chartok_3d_seq,\n",
    "                'nodes_3d_dict': nodes_3d_dict,\n",
    "                'original_afm_sample': afm_sample,\n",
    "                'coords_3d': torch.tensor(nodes_3d_dict['coords_3d'], dtype=torch.float32),\n",
    "                'symbols': nodes_3d_dict['symbols']\n",
    "            }\n",
    "        else:\n",
    "            # Fallback\n",
    "            converted_sample = {\n",
    "                'idx': idx,\n",
    "                'chartok_coords_3d': torch.tensor([1, 2]),\n",
    "                'nodes_3d_dict': {'coords_3d': [], 'symbols': []},\n",
    "                'original_afm_sample': afm_sample,\n",
    "                'coords_3d': torch.tensor([]),\n",
    "                'symbols': []\n",
    "            }\n",
    "        \n",
    "        self.cache[idx] = converted_sample\n",
    "        return converted_sample\n",
    "\n",
    "class AFM3DCollator:\n",
    "    \"\"\"Custom collate function for 3D AFM data\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        \n",
    "        sequences = [sample['chartok_coords_3d'] for sample in batch]\n",
    "        coords_3d_list = [sample['coords_3d'] for sample in batch]\n",
    "        symbols_list = [sample['symbols'] for sample in batch]\n",
    "        \n",
    "        # Pad sequences\n",
    "        max_seq_len = min(max(len(seq) for seq in sequences), self.max_length)\n",
    "        padded_sequences = torch.zeros((batch_size, max_seq_len), dtype=torch.long)\n",
    "        attention_masks = torch.zeros((batch_size, max_seq_len), dtype=torch.bool)\n",
    "        \n",
    "        # 3D coordinates\n",
    "        max_atoms = max(len(coords) for coords in coords_3d_list) if coords_3d_list[0].numel() > 0 else 1\n",
    "        coords_3d_batch = torch.zeros((batch_size, max_atoms, 3))\n",
    "        coords_mask = torch.zeros((batch_size, max_atoms), dtype=torch.bool)\n",
    "        \n",
    "        for i, (seq, coords_3d, symbols) in enumerate(zip(sequences, coords_3d_list, symbols_list)):\n",
    "            seq_len = min(len(seq), max_seq_len)\n",
    "            padded_sequences[i, :seq_len] = seq[:seq_len]\n",
    "            attention_masks[i, :seq_len] = True\n",
    "            \n",
    "            if coords_3d.numel() > 0 and len(coords_3d.shape) == 2:\n",
    "                atom_len = min(len(coords_3d), max_atoms)\n",
    "                coords_3d_batch[i, :atom_len] = coords_3d[:atom_len]\n",
    "                coords_mask[i, :atom_len] = True\n",
    "        \n",
    "        return {\n",
    "            'input_ids': padded_sequences,\n",
    "            'attention_mask': attention_masks,\n",
    "            'coords_3d': coords_3d_batch,\n",
    "            'coords_mask': coords_mask,\n",
    "            'symbols_batch': symbols_list,\n",
    "            'batch_size': batch_size,\n",
    "            'max_seq_len': max_seq_len,\n",
    "            'max_atoms': max_atoms\n",
    "        }\n",
    "\n",
    "def make_collate_fn(tokenizer):\n",
    "    \"\"\"Create a collate function that maps out-of-range tokens to UNK and batches coords/edges.\"\"\"\n",
    "    vocab_size = len(tokenizer)\n",
    "    base_vocab = len(tokenizer.stoi)\n",
    "\n",
    "    def _collate(batch):\n",
    "        # indices and images\n",
    "        indices = torch.stack([item['idx'] for item in batch])\n",
    "        images = torch.stack([item['image'] for item in batch])\n",
    "\n",
    "        # sequences with UNK clamping\n",
    "        raw_sequences = [item['sequence'] for item in batch]\n",
    "        sequences = []\n",
    "        seq_lengths = []\n",
    "        for seq in raw_sequences:\n",
    "            seq = seq.clone()\n",
    "            invalid = (seq >= vocab_size) | (seq < 0)\n",
    "            if invalid.any():\n",
    "                seq[invalid] = UNK_ID\n",
    "            sequences.append(seq)\n",
    "            seq_lengths.append(torch.tensor(len(seq), dtype=torch.long))\n",
    "        seq_lengths = torch.stack(seq_lengths)\n",
    "\n",
    "        # debug token range for first few batches\n",
    "        all_tokens = torch.cat(sequences) if len(sequences) > 0 else torch.tensor([], dtype=torch.long)\n",
    "        if all_tokens.numel() > 0:\n",
    "            max_token = all_tokens.max().item(); min_token = all_tokens.min().item()\n",
    "        else:\n",
    "            max_token = min_token = 0\n",
    "        if not hasattr(_collate, 'call_count'):\n",
    "            _collate.call_count = 0\n",
    "        _collate.call_count += 1\n",
    "        if _collate.call_count <= 3:\n",
    "            logger.info(f\"🔍 Batch {_collate.call_count}: Token range [{min_token}, {max_token}], base_vocab={base_vocab}, total_vocab={vocab_size}, sequences={len(sequences)}\")\n",
    "\n",
    "        # pad sequences\n",
    "        max_len = max(len(seq) for seq in sequences) if sequences else 0\n",
    "        batch_size = len(batch)\n",
    "        padded_sequences = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            L = len(seq)\n",
    "            if L > 0:\n",
    "                padded_sequences[i, :L] = seq\n",
    "\n",
    "        # coords_3d batching (pad to max atoms), use -100 for masked positions\n",
    "        coords_list = [item['coords_3d'] for item in batch]\n",
    "        num_atoms_list = [int(item['num_atoms']) for item in batch]\n",
    "        max_atoms = max(num_atoms_list) if num_atoms_list else 0\n",
    "        coords_3d_batch = torch.full((batch_size, max_atoms, 3), -100.0, dtype=torch.float32)\n",
    "        for i, coords in enumerate(coords_list):\n",
    "            n = min(coords.size(0), max_atoms)\n",
    "            if n > 0:\n",
    "                coords_3d_batch[i, :n, :] = coords[:n, :]\n",
    "\n",
    "        # atom_indices: positions of symbol tokens in sequence: [1..num_atoms]\n",
    "        atom_indices = torch.zeros(batch_size, max_atoms, dtype=torch.long)\n",
    "        for i, n_atoms in enumerate(num_atoms_list):\n",
    "            n = min(n_atoms, max_atoms)\n",
    "            if n > 0:\n",
    "                atom_indices[i, :n] = torch.arange(1, 1 + n)\n",
    "\n",
    "        # edges target placeholder: ignore all pairs by default (-100), shape [B, max_atoms, max_atoms]\n",
    "        edges_target = torch.full((batch_size, max_atoms, max_atoms), -100, dtype=torch.long)\n",
    "\n",
    "        refs = {\n",
    "            'chartok_coords': (padded_sequences, seq_lengths),\n",
    "            'coords_3d': coords_3d_batch,\n",
    "            'edges': edges_target,\n",
    "            'atom_indices': (atom_indices,),\n",
    "        }\n",
    "        return indices, images, refs\n",
    "\n",
    "    return _collate\n",
    "\n",
    "def create_3d_dataloader(data_path, batch_size=16, num_workers=4):\n",
    "    \"\"\"Create DataLoader for 3D AFM training from HDF5 using the provided pipeline\"\"\"\n",
    "    args = create_model_args()\n",
    "\n",
    "    # Determine HDF5 file path\n",
    "    h5_path = data_path\n",
    "    if os.path.isdir(data_path):\n",
    "        # pick the first .h5 file in the directory\n",
    "        h5_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.h5') or f.endswith('.hdf5')]\n",
    "        if len(h5_files) == 0:\n",
    "            raise FileNotFoundError(f\"No .h5/.hdf5 files found under {data_path}\")\n",
    "        h5_path = sorted(h5_files)[0]\n",
    "\n",
    "    # Build tokenizer(s)\n",
    "    tokenizers = get_tokenizer(args)\n",
    "    chartok_tok = tokenizers['chartok_coords']\n",
    "    logger.info(f\"📚 Tokenizer built: chartok_coords | base_vocab={len(chartok_tok.stoi)} | total_vocab={len(chartok_tok)} | coord_bins={args.coord_bins}\")\n",
    "\n",
    "    # Create dataset from HDF5\n",
    "    dataset = H5AFM3DDataset(h5_path, chartok_tok, coord_bins=args.coord_bins)\n",
    "    logger.info(f\"📂 Using AFM HDF5 dataset: {h5_path} | {len(dataset)} samples\")\n",
    "\n",
    "    # Collate function\n",
    "    collate_fn = make_collate_fn(chartok_tok)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    logger.info(f\"✅ DataLoader created with {len(dataset)} samples, {len(train_loader)} batches\")\n",
    "    return train_loader, tokenizers, dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d7d95-c2d0-457d-a4fb-0dd794755ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolNexTR_env",
   "language": "python",
   "name": "molnextr_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
